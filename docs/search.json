[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Statistics with R",
    "section": "",
    "text": "Introduction\nslack channel",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#required-packages",
    "href": "index.html#required-packages",
    "title": "Spatial Statistics with R",
    "section": "Required packages",
    "text": "Required packages\nThe following packages may be used during the course; it is assumed that you know how to install packages, and have permission to do so on your computer.\nCRAN packages:\n\ninstall.packages(c(\"classInt\",\n\"colorspace\",\n\"dplyr\",\n\"ggplot2\",\n\"gstat\",\n\"hglm\",\n\"igraph\",\n\"lme4\",\n\"lwgeom\",\n\"maps\" ,\n\"mapview\",\n\"randomForest\",\n\"rnaturalearth\",\n\"s2\",\n\"scales\",\n\"sf\",\n\"sp\",\n\"spacetime\",\n\"spdep\",\n\"spatialreg\",\n\"spatstat\",\n\"spData\",\n\"stars\",\n\"terra\",\n\"tidyverse\",\n\"tmap\",\n\"units\",\n\"viridis\",\n\"viridisLite\",\n\"xts\"))\n\nnon-CRAN packages:\n\ninstall.packages(\"spDataLarge\", repos = \"https://nowosad.github.io/drat/\", \n                 type = \"source\") # 23 Mb\ninstall.packages(\"starsdata\", repos = \"http://cran.uni-muenster.de/pebesma/\", \n                 type = \"source\") # 1 Gb\n\nIntroduction to the course\n\nintroduction of the tutor\nintroduction of course participants, please state\n\nname,\nwhere you’re from,\nwhat kind of spatial data analysis you have done so far\n\n\nHow we work\nLive sessions are from 14:00-18:00 CET (Berlin time); daily schedule:\n\n14:00 - 14:45 lecture\n14:45 - 15:30 practical exercises (break-out groups)\n15:30 - 15:45 discussion of exercises\n15:45 - 16:15 break\n16:15 - 17:00 lecture\n17:00 - 17:45 practical exercises (break-out groups)\n17:45 - 18:00 discussion of exercises\n\nFurther:\n\nplease raise hands or speak up whenever something comes up\nslack communication during the full week\nplease share questions you run into in your actual research, preferably with (example) data and R code\nplease use the open channels in slack, so that everyone can learn from q + a’s\nResources\n\n\nSpatial Data Science: With applications in R, by Pebesma and Bivand 2023 (open online, hard copy from CRC)\nVignettes of sf: tab “Articles”\nVignettes of stars: tab “Articles”\nAll these material are written using quarto or R-markdown",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#why-r-for-spatial-statistics",
    "href": "index.html#why-r-for-spatial-statistics",
    "title": "Spatial Statistics with R",
    "section": "Why R for spatial statistics?",
    "text": "Why R for spatial statistics?\n\nR is old! Think of the advantages!\nR is as good as any data science language, but is more in focus with the statistical community\nMost researchers in spatial statistics who share code have used or use R\nR has a strong ecosystem of users and developers, who communicate and collaborate (and compete, mostly in a good way)\nR spatial packages have gone full cycle:\n\nthe first generation has been deprecated (rgdal, rgeos, maptools),\nthen removed from CRAN, and\nsuperseded by modern versions (sf and stars replaced sp, terra replaced raster)\n\n\nR is a data science language that allows you to work reproducibly\n\nBecause we have CRAN and CRAN Taskviews: Spatial, SpatioTemporal, Tracking\n\n\nReproducing or recreating the current course\n\nGo to https://github.com/edzer/sswr/\n\nGo to “Code”, then “copy URL to clipboard”\nClone this repo to your hard drive\nStart RStudio by double clickign the sswr.Rproj file in the source directory\nReproduce these course materials by installing quarto and\n\nin RStudio: run build - render book, or\non the command line: run quarto render in the course directory\n\n\nRun individual code sections in RStudio, and modify them!",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#exercises",
    "href": "index.html#exercises",
    "title": "Spatial Statistics with R",
    "section": "Exercises",
    "text": "Exercises\n\nInstall the spDataLarge package (see instructions above)\nCopy the course material from GitHub to your local machine\nOpen it in RStudio\nOpen the day1.qmd file. Try to identify a code chunk.\nRun the first code chunk.\nSkip to the last code chunk; run all code chunks above it (by a single click), and then run this last code chunk.\nRender the entire course “book”, view the result by opening _book/index.html in a web browser (from Rstudio)",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "day1.html",
    "href": "day1.html",
    "title": "\n1  Introduction to spatial data\n",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day1.html#what-is-special-about-spatial-data",
    "href": "day1.html#what-is-special-about-spatial-data",
    "title": "\n1  Introduction to spatial data\n",
    "section": "\n1.1 What is special about spatial data?",
    "text": "1.1 What is special about spatial data?\n\n\nLocation. Does location always involve coordinates? Relative/absolute, qualitative/quantitative\n\nCoordinates. What are coordinates? Dimension(s), unit\n\nTime. If not explicit, there is an implicit time reference. Dimension(s), unit, datetime\n\nAttributes. at specific locations we measure (observe) specific properties\nQuite often, we want to know where things change (space-time interactions).\n\nReference systems for space, time, and attributes: what are they?\n\nSupport: if we have an attribute value associated with a line, polygon or grid cell:\n\ndoes the value summarise all values at points? (line/area/cell support), or\nis the value constant throughout the line/area/cell (point support)?\n\n\n\nContinuity:\n\nis a variable spatially continuous? Yes for geostatitical data, no for point patterns\nis an attribute variable continuous? Stevens’s measurement scales: yes if Interval or Ratio.\n\n\n\nSupport: examples\n\nRoad properties\n\nroad type: gravel, brick, asphalt (point support: everywhere on the whole road)\nmean width: block support (summary value)\nminimum width: block support (although the minimum width may be the value at a single (point) location, it summarizes all widths of the road–we no longer know the width at any specific point location)\n\n\nLand use/land cover\n\nwhen we classify e.g. 30 m x 30 m Landsat pixels into a single class, this single class is not constant throughout this pixel\nroad type is a land cover type, but a road never covers a 30 m x 30 m pixel\na land cover type like “urban” is associated with a positive (non-point) support: we don’t say a point in a garden or park is urban, or a point on a roof, but these are part of a (block support) urban fabric\n\n\nElevation\n\nin principle, we can measure elevation at a point; in practice, every measuring device has a physical (non-point) size\n\n\nFurther reading: Chapter 5: Attributes and Support",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day1.html#spatial-vs.-geospatial",
    "href": "day1.html#spatial-vs.-geospatial",
    "title": "\n1  Introduction to spatial data\n",
    "section": "\n1.2 Spatial vs. Geospatial",
    "text": "1.2 Spatial vs. Geospatial\n\nSpatial refers (physical) spaces, 2- or 3-dimensional (\\(R^2\\) or \\(R^3\\))\n\nMost often spatial statistics considers 2-dimensional problems\n3-d: meteorology, climate science, geophysics, groundwater hydrology, aeronautics, …\n\n\n“Geo” refers to the Earth\nFor Earth coordinates, we always need a datum, consisting of an ellipsoid (shape) and the way it is fixed to the Earth (origin)\n\nThe Earth is modelled by an ellipsoid, which is nearly round\nIf we consider Earth-bound areas as flat, for larger areas we get the distances wrong\nWe can (and do) also work on \\(S^2\\), the surface of a sphere, rather than \\(R^2\\), to get distances right, but this creates a number of challenges (such as plotting on a 2D device)\n\n\nNon-geospatial spaces could be:\n\nAssociated with other bodies (moon, Mars)\nAstrophysics, places/directions in the universe\nLocations in a building (where we use “engineering coordinates”, relative to a building corner and orientation)\nMicroscope images\nMRT scans (3-D), places in a human body\nlocations on a genome?\n\n\n\n\nCodelibrary(rnaturalearth)\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\npar(mar = c(2,2,0,0) + .1)\nne_countries() |&gt; st_geometry() |&gt; plot(axes=TRUE)\n\n\n\nworld map, with longitude and latitude map linearly to x and y (Plate Caree)\n\n\n\n\n\n\n\n\n\nWhat is Statistics\n\n\n\n… or what are statistics?\n\nstatistic: singular; a descriptive measure summarising some data\nstatistics: plural; “minimum and maximum are two statistics”\nStatistics: singular; a scientific discipline aiming at modelling data, using probability theory\n\nwhere does randomness come from? Design-based vs. model-based\nare parameters random or fixed? Bayesian vs. frequentist\ninference, prediction, simulations\n\n\nTypical approach: observation = signal + noise, noise modelled by random variables\n\n\n\nDesign-based statistics\nIn design-based statistics, randomness comes from random sampling. Consider an area \\(B\\), from which we take samples \\[z(s),\ns \\in B,\\] with \\(s\\) a location for instance two-dimensional: \\(s_i =\n\\{x_i,y_i\\}\\). If we select the samples randomly, we can consider \\(S \\in B\\) a random variable, and \\(z(S)\\) a random sample. Note the randomness in \\(S\\), not in \\(z\\).\nTwo variables \\(z(S_1)\\) and \\(z(S_2)\\) are independent if \\(S_1\\) and \\(S_2\\) are sampled independently. For estimation we need to know the inclusion probabilities, which need to be non-negative for every location.\nIf inclusion probabilities are constant (simple random sampling; or complete spatial randomness: day 2, point patterns) then we can estimate the mean of \\(Z(B)\\) by the sample mean \\[\\frac{1}{n}\\sum_{j=1}^n\nz(s_j).\\] This also predicts the value of a randomly chosen observation \\(z(S)\\). It cannot be used to predict the value \\(z(s_0)\\) for a non-randomly chosen location \\(s_0\\); for this we need a model.\nModel-based statistics\nModel-based statistics assumes randomness in the measured responses; consider a regression model \\(y = X\\beta + e\\), where \\(e\\) is a random variable and as a consequence \\(y\\), the response variable is a random variable. In the spatial context we replace \\(y\\) with \\(z\\), and capitalize it to indicate it is a random variable, and write \\[Z(s) = X(s)\\beta + e(s)\\] to stress that\n\n\n\\(Z(s)\\) is a random function (random variables \\(Z\\) as a function of \\(s\\))\n\n\\(X(s)\\) is the matrix with covariates, which depend on \\(s\\)\n\n\n\\(\\beta\\) are (spatially) constant coefficients, not depening on \\(s\\)\n\n\n\\(e(s)\\) is a random function with mean zero and covariance matrix \\(\\Sigma\\)\n\n\nIn the regression literature this is called a (linear) mixed model, because \\(e\\) is not i.i.d. If \\(e(s)\\) contains an iid component \\(\\epsilon\\) we can write this as\n\\[Z(s) = X(s)\\beta + w(s) + \\epsilon\\]\nwith \\(w(s)\\) the spatial signal, and \\(\\epsilon\\) a noise compenent e.g. due to measurement error.\nPredicting \\(Z(s_0)\\) will involve (GLS) estimation of \\(\\beta\\), but also prediction of \\(e(s_0)\\) using correlated, nearby observations (day 3: geostatistics).\nDesign- or model-based?\n\ndesign-based requires a random sample, if that is the case it needs no further assumptions\nmodel-based requires stationarity assumptions to estimate \\(\\Sigma\\)\n\nmodel-based is typically more effective for interpolation problems\ndesign-based can be most effective when estimating, e.g. average mapping errors\nUsing coordinates as covariates?\n\n(day 4)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day1.html#spatial-statistics-data-types",
    "href": "day1.html#spatial-statistics-data-types",
    "title": "\n1  Introduction to spatial data\n",
    "section": "\n1.3 Spatial statistics: data types",
    "text": "1.3 Spatial statistics: data types\nPoint Patterns\n\nPoints (locations) + observation window\nExample from here\n\n\n\n\n\n\n\n\n\nFigure 1.1: Wind turbine parks in Germany\n\n\n\n\n\nThe locations contain the information\nPoints may have (discrete or continuous) marks (attributes)\nThe observation window is, apart from the points, empty\n\nGeostatistical data: locations + measured values\n\nCodelibrary(sf)\nno2 &lt;- read.csv(system.file(\"external/no2.csv\",\n    package = \"gstat\"))\ncrs &lt;- st_crs(\"EPSG:32632\")\nst_as_sf(no2, crs = \"OGC:CRS84\", coords =\n    c(\"station_longitude_deg\", \"station_latitude_deg\")) |&gt;\n    st_transform(crs) -&gt; no2.sf\nlibrary(ggplot2)\n# plot(st_geometry(no2.sf))\n\"https://github.com/edzer/sdsr/raw/main/data/de_nuts1.gpkg\" |&gt;\n  read_sf() |&gt;\n  st_transform(crs) -&gt; de\nggplot() + geom_sf(data = de) +\n    geom_sf(data = no2.sf, mapping = aes(col = NO2))\n\n\n\nNO2 measurements at rural background stations (EEA)\n\n\n\n\nThe value of interest is measured at a set of sample locations\nAt other location, this value exists but is missing\n\nThe interest is in estimating (predicting) this missing value (interpolation)\nThe actual sample locations are not of (primary) interest, the signal is in the measured values\nAreal data\n\npolygons (or grid cells) + polygon summary values\n\n\nCode# https://en.wikipedia.org/wiki/List_of_NUTS_regions_in_the_European_Union_by_GDP\nde$GDP_percap = c(45200, 46100, 37900, 27800, 49700, 64700, 45000, 26700, 36500, 38700, 35700, 35300, 29900, 27400, 32400, 28900)\nggplot() + geom_sf(data = de) +\n    geom_sf(data = de, mapping = aes(fill = GDP_percap)) + \n    geom_sf(data = st_cast(de, \"MULTILINESTRING\"), col = 'white')\n\n\n\nNO2 rural background, average values per NUTS1 region\n\n\n\n\nThe polygons contain polygon summary (polygon support) values, not values that are constant throughout the polygon (as in a soil, lithology or land cover map)\nNeighbouring polygons are typically related: spatial correlation\nneighbour-neighbour correlation: Moran’s I\nregression models with correlated errors, spatial lag models, CAR models, GMRFs, …\nsee Ch 14-17 of SDSWR\n\nbriefly addressed on Friday",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day1.html#data-types-that-received-less-attention-in-the-spatial-statistics-literature",
    "href": "day1.html#data-types-that-received-less-attention-in-the-spatial-statistics-literature",
    "title": "\n1  Introduction to spatial data\n",
    "section": "\n1.4 Data types that received less attention in the spatial statistics literature",
    "text": "1.4 Data types that received less attention in the spatial statistics literature\nImage data\n\nCodelibrary(stars)\n# Loading required package: abind\nplot(L7_ETMs, rgb = 1:3)\n\n\n\nRGB image from a Landsat scene\n\n\n\n\nare these geostatistical data, or areal data?\nIf we identify objects from images, can we see them as point patterns?\nTracking data, trajectories\n\nCode# from: https://r-spatial.org/r/2017/08/28/nest.html\nlibrary(tidyverse)\n# ── Attaching core tidyverse packages ──────────── tidyverse 2.0.0 ──\n# ✔ dplyr     1.1.4     ✔ readr     2.1.5\n# ✔ forcats   1.0.0     ✔ stringr   1.5.1\n# ✔ lubridate 1.9.3     ✔ tibble    3.2.1\n# ✔ purrr     1.0.2     ✔ tidyr     1.3.1\n# ── Conflicts ────────────────────────────── tidyverse_conflicts() ──\n# ✖ dplyr::filter() masks stats::filter()\n# ✖ dplyr::lag()    masks stats::lag()\n# ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nstorms.sf &lt;- storms %&gt;%\n    st_as_sf(coords = c(\"long\", \"lat\"), crs = 4326)\nstorms.sf &lt;- storms.sf %&gt;% \n    mutate(time = as.POSIXct(paste(paste(year,month,day, sep = \"-\"), \n                                   paste(hour, \":00\", sep = \"\")))) %&gt;% \n    select(-month, -day, -hour)\nstorms.nest &lt;- storms.sf %&gt;% group_by(name, year) %&gt;% nest\nto_line &lt;- function(tr) st_cast(st_combine(tr), \"LINESTRING\") %&gt;% .[[1]] \ntracks &lt;- storms.nest %&gt;% pull(data) %&gt;% map(to_line) %&gt;% st_sfc(crs = 4326)\nstorms.tr &lt;- storms.nest %&gt;% select(-data) %&gt;% st_sf(geometry = tracks)\nstorms.tr %&gt;% ggplot(aes(color = year)) + geom_sf()\n\n\n\nStorm/hurricane trajectories colored by year\n\n\n\n\nA temporal snapshot (time slice) of a set of moving things forms a point pattern\nWe often analyse trajectories by\n\nestimating densities, for space-time blocks, per individual or together\nanalysing interactions (alibi problem, mating animals, home range, UDF etc)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day1.html#checklist-if-you-have-spatial-data",
    "href": "day1.html#checklist-if-you-have-spatial-data",
    "title": "\n1  Introduction to spatial data\n",
    "section": "\n1.5 Checklist if you have spatial data",
    "text": "1.5 Checklist if you have spatial data\n\nDo you have the spatial coordinates of your data?\nAre the coordinates Earth-bound?\nIf yes, do you have the coordinate reference system of them?\nWhat is the support (physical size) of your observations?\nWere the data obtained by random sampling, and if yes, do you have sampling weights?\nDo you know the extent (\\(B\\)) from which your data were sampled, or collected?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day1.html#exercises",
    "href": "day1.html#exercises",
    "title": "\n1  Introduction to spatial data\n",
    "section": "\n1.6 Exercises",
    "text": "1.6 Exercises\n\nWhat is the coordinate reference system of the ne_countries() dataset, imported above?\nLook up the “Equidistant Cylindrical (Plate Carrée)” projection on the https://proj.org website.\nWhy is this projection called The simplest of all projections?\nProject ne_countries to Plate Carrée, and plot it with axes=TRUE. What has changed? (Hint: st_crs() accepts a proj string to define a coordinate reference system (CRS); st_transform() transforms a dataset to a new CRS.)\nProject the same dataset to Eckert IV projection. What has changed?\nAlso try plotting this dataset after transforming it to an orthographic projection with +proj=ortho; what went wrong?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day1.html#further-reading",
    "href": "day1.html#further-reading",
    "title": "\n1  Introduction to spatial data\n",
    "section": "\n1.7 Further reading",
    "text": "1.7 Further reading\n\nRipley, B. 1981. Spatial Statistics. Wiley.\nCressie, N. 1993. Statistics for Spatial Data. Wiley.\nCochran, W.G. 1977. Sampling Techniques. Wiley.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction to spatial data</span>"
    ]
  },
  {
    "objectID": "day2.html",
    "href": "day2.html",
    "title": "\n2  Point Pattern data\n",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#intro-to-sf-and-stars",
    "href": "day2.html#intro-to-sf-and-stars",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.1 Intro to sf and stars\n",
    "text": "2.1 Intro to sf and stars\n\n\nBriefly: sf provides classes and methods for simple features\n\na feature is a “thing”, with geometrical properties (point(s), line(s), polygon(s)) and attributes\n\nsf stores data in data.frames with a list-column (of class sfc) that holds the geometries\n\n\n\n\n\n\n\n\n\nthe Simple Feature standard\n\n\n\n“Simple Feature Access” is an open standard for data with vector geometries. It defines a set of classes for geometries and operations on them.\n\n“simple” refers to curves that are “simply” represented by points connected by straight lines\nconnecting lines are not allowed to self-intersect\n\npolygons can have holes, and have validity constraints: holes cannot extrude the outer ring etc.\nAll spatial software uses this: ArcGIS, QGIS, PostGIS, other spatial databases, …\n\n\n\nWhy do all functions in sf start with st_?\n\nsee here\n\nThe larger geospatial open source ecosystem\nR and beyond:\n\n\n\n\n\n\n\nFigure 2.1: sf and its dependencies; arrows indicate strong dependency, dashed arrows weak dependency\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.2: upstream dependencies on OSGEO and other C/C++ libraries in R and Python spatial data sciece packages\n\n\n\n\n\nsf operators, how to understand?\nsf has objects at three nested “levels”:\n\nsfg: a single geometry (without coordinate reference system)\nsfc: a set of sfg geometries, with a coordinate reference system and bounding box\nsf: a data.frame or tibble with at least one geometry (sfc) column\n\nOperations not involving geometry (data.frame; base R; tidyverse)\n\ngeometry column + sf class is sticky!\nthis can be convenient, and sometimes annoying\nuse as.data.frame or as_tibble to strip the sf class label\n\n\n\nOperations involving only geometry\n\n\npredicates (resulting TRUE/FALSE)\n\nunary\nbinary: DE9-IM; work on two sets, result sgbp, which is a sparse logical matrix representation\n\nis_within_distance\n\n\n\n\n\nmeasures\n\nunary: length, area\nbinary: distance, by_element = FALSE\n\n\n\n\ntransformers\n\nunary: buffer, centroid\nbinary: intersection, union, difference, symdifference\nn-ary: intersection, difference\n\n\n\n\n\nOperations involving geometry and attributes\n\nmany of the above!\nst_join\naggregate\n\nst_interpolate_aw: requires expression whether variable is spatially extensive or intensive",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#sf-and-spatstat",
    "href": "day2.html#sf-and-spatstat",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.2 sf and spatstat\n",
    "text": "2.2 sf and spatstat\n\nWe can try to convert an sf object to a ppp (point pattern object in spatstat):\n\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\nlibrary(spatstat)\n# Loading required package: spatstat.data\n# Loading required package: spatstat.univar\n# spatstat.univar 3.0-1\n# Loading required package: spatstat.geom\n# spatstat.geom 3.3-3\n# Loading required package: spatstat.random\n# spatstat.random 3.3-2\n# Loading required package: spatstat.explore\n# Loading required package: nlme\n# spatstat.explore 3.3-2\n# Loading required package: spatstat.model\n# Loading required package: rpart\n# spatstat.model 3.3-2\n# Loading required package: spatstat.linnet\n# spatstat.linnet 3.2-2\n# \n# spatstat 3.2-1 \n# For an introduction to spatstat, type 'beginner'\ndemo(nc, echo = FALSE, ask = FALSE)\npts = st_centroid(st_geometry(nc))\nas.ppp(pts) # ???\n# Error: Only projected coordinates may be converted to spatstat\n# class objects\n\nNote that sf interprets a NA CRS as: flat, projected (Cartesian) space.\nWhy is this important?\n\n(p1 = st_point(c(0, 0)))\n# POINT (0 0)\n(p2 = st_point(c(1, 0)))\n# POINT (1 0)\nst_distance(p1, p2)\n#      [,1]\n# [1,]    1\nst_sfc(p1, crs = 'OGC:CRS84')\n# Geometry set for 1 feature \n# Geometry type: POINT\n# Dimension:     XY\n# Bounding box:  xmin: 0 ymin: 0 xmax: 0 ymax: 0\n# Geodetic CRS:  WGS 84 (CRS84)\n# POINT (0 0)\nst_distance(st_sfc(p1, crs = 'OGC:CRS84'), st_sfc(p2, crs = 'OGC:CRS84'))\n# Units: [m]\n#        [,1]\n# [1,] 111195\n(p1 = st_point(c(0, 80)))\n# POINT (0 80)\n(p2 = st_point(c(1, 80)))\n# POINT (1 80)\nst_distance(p1, p2)\n#      [,1]\n# [1,]    1\nst_distance(st_sfc(p1, crs = 'OGC:CRS84'), st_sfc(p2, crs = 'OGC:CRS84'))\n# Units: [m]\n#       [,1]\n# [1,] 19309\n\nAlso areas:\n\np = st_as_sfc(\"POLYGON((0 80, 120 80, 240 80, 0 80))\")\nst_area(p)\n# [1] 0\nst_area(st_sfc(p, crs = 'OGC:CRS84')) |&gt; units::set_units(km^2)\n# 1620544 [km^2]\npole = st_as_sfc(\"POINT(0 90)\")\nst_intersects(pole, p)\n# Sparse geometry binary predicate list of length 1, where the\n# predicate was `intersects'\n#  1: (empty)\nst_intersects(st_sfc(pole, crs = 'OGC:CRS84'), st_sfc(p, crs = 'OGC:CRS84'))\n# Sparse geometry binary predicate list of length 1, where the\n# predicate was `intersects'\n#  1: 1\n\nWhat to do with nc? Project to \\(R^2\\) (flat space):\n\nnc |&gt; st_transform('EPSG:32119') |&gt; st_centroid() -&gt; pts\n# Warning: st_centroid assumes attributes are constant over\n# geometries\npts\n# Simple feature collection with 100 features and 14 fields\n# Attribute-geometry relationships: constant (6), aggregate (8)\n# Geometry type: POINT\n# Dimension:     XY\n# Bounding box:  xmin: 149000 ymin: 36500 xmax: 898000 ymax: 306000\n# Projected CRS: NAD83 / North Carolina\n# First 10 features:\n#     AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID\n# 1  0.114      1.44  1825    1825        Ashe 37009  37009        5\n# 2  0.061      1.23  1827    1827   Alleghany 37005  37005        3\n# 3  0.143      1.63  1828    1828       Surry 37171  37171       86\n# 4  0.070      2.97  1831    1831   Currituck 37053  37053       27\n# 5  0.153      2.21  1832    1832 Northampton 37131  37131       66\n# 6  0.097      1.67  1833    1833    Hertford 37091  37091       46\n# 7  0.062      1.55  1834    1834      Camden 37029  37029       15\n# 8  0.091      1.28  1835    1835       Gates 37073  37073       37\n# 9  0.118      1.42  1836    1836      Warren 37185  37185       93\n# 10 0.124      1.43  1837    1837      Stokes 37169  37169       85\n#    BIR74 SID74 NWBIR74 BIR79 SID79 NWBIR79                  geom\n# 1   1091     1      10  1364     0      19  POINT (385605 3e+05)\n# 2    487     0      10   542     3      12 POINT (419198 306144)\n# 3   3188     5     208  3616     6     260 POINT (458418 296669)\n# 4    508     1     123   830     2     145 POINT (876266 298782)\n# 5   1421     9    1066  1606     3    1197 POINT (752184 297618)\n# 6   1452     7     954  1838     5    1237 POINT (789602 291533)\n# 7    286     0     115   350     2     139 POINT (857738 297588)\n# 8    420     0     254   594     2     371 POINT (815437 301289)\n# 9    968     4     748  1190     2     844 POINT (689435 294013)\n# 10  1612     1     160  2038     5     176 POINT (498892 294730)\n(pp = as.ppp(pts))\n# Marked planar point pattern: 100 points\n# Mark variables: \n#    AREA PERIMETER CNTY_ CNTY_ID NAME FIPS FIPSNO CRESS_ID BIR74 \n# SID74 NWBIR74 BIR79 SID79 NWBIR79\n# window: rectangle = [148701, 898181] x [36519, 306144] units\nst_as_sf(pp)\n# Simple feature collection with 101 features and 15 fields\n# Geometry type: GEOMETRY\n# Dimension:     XY\n# Bounding box:  xmin: 149000 ymin: 36500 xmax: 898000 ymax: 306000\n# CRS:           NA\n# First 10 features:\n#     AREA PERIMETER CNTY_ CNTY_ID        NAME  FIPS FIPSNO CRESS_ID\n# NA    NA        NA    NA      NA        &lt;NA&gt;  &lt;NA&gt;     NA       NA\n# 1  0.114      1.44  1825    1825        Ashe 37009  37009        5\n# 2  0.061      1.23  1827    1827   Alleghany 37005  37005        3\n# 3  0.143      1.63  1828    1828       Surry 37171  37171       86\n# 4  0.070      2.97  1831    1831   Currituck 37053  37053       27\n# 5  0.153      2.21  1832    1832 Northampton 37131  37131       66\n# 6  0.097      1.67  1833    1833    Hertford 37091  37091       46\n# 7  0.062      1.55  1834    1834      Camden 37029  37029       15\n# 8  0.091      1.28  1835    1835       Gates 37073  37073       37\n# 9  0.118      1.42  1836    1836      Warren 37185  37185       93\n#    BIR74 SID74 NWBIR74 BIR79 SID79 NWBIR79  label\n# NA    NA    NA      NA    NA    NA      NA window\n# 1   1091     1      10  1364     0      19  point\n# 2    487     0      10   542     3      12  point\n# 3   3188     5     208  3616     6     260  point\n# 4    508     1     123   830     2     145  point\n# 5   1421     9    1066  1606     3    1197  point\n# 6   1452     7     954  1838     5    1237  point\n# 7    286     0     115   350     2     139  point\n# 8    420     0     254   594     2     371  point\n# 9    968     4     748  1190     2     844  point\n#                              geom\n# NA POLYGON ((148701 36519, 898...\n# 1            POINT (385605 3e+05)\n# 2           POINT (419198 306144)\n# 3           POINT (458418 296669)\n# 4           POINT (876266 298782)\n# 5           POINT (752184 297618)\n# 6           POINT (789602 291533)\n# 7           POINT (857738 297588)\n# 8           POINT (815437 301289)\n# 9           POINT (689435 294013)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#exercises",
    "href": "day2.html#exercises",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.3 Exercises",
    "text": "2.3 Exercises\n\nCompute the distance between POINT(10 -90) and POINT(50 -90), assuming (i) these are coordinates in a Cartesian space, and (ii) these are geodetic coordinates. What are the units of the result?\nLoad the nc dataset into your session (e.g. using library(sf); demo(nc)) and convert it into a stars object using (i) st_as_stars(), (ii) st_rasterize(), (iii) st_interpolate_aw()\n\nLoad the L7_ETMs dataset into your session (e.g. using library(stars); L7_ETMs = st_as_stars(L7_ETMs)), and convert the object to an sf object (i) using st_as_sf(), (ii) using st_as_sf(..., as_points = TRUE), and explain the differences (also plot the resulting sf objects). Randomly sample 100 points from the bounding box of L7_ETMs, and extract the image values at these points using st_extract(), and convert the result into an sf object.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#intro-to-spatstat",
    "href": "day2.html#intro-to-spatstat",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.4 Intro to spatstat\n",
    "text": "2.4 Intro to spatstat\n\nConsider a point pattern that consist of\n\na set of known coordinates\nan observation window\n\nWe can ask ourselves: our point pattern be a realisation of a completely spatially random (CSR) process? A CSR process has\n\na spatially constant intensity (mean: first order property)\ncompletely independent locations (interactions: second order property)\n\ne.g.\n\nlibrary(spatstat)\nset.seed(13431)\nCSR = rpoispp(100)\nplot(CSR)\n\n\n\n\n\n\n\nOr does it have a non-constant intensity, but otherwise independent points?\n\nppi = rpoispp(function(x,y,...) 500 * x)\nplot(ppi, main = \"inhomogeneous\")\n\n\n\n\n\n\n\nOr does it have constant intensity, but dependent points:\n\ncl &lt;- rThomas(100, .02, 5)\nplot(cl, main = \"clustered\")\n\n\n\n\n\n\n\n\nhc &lt;- rHardcore(0.05,1.5,square(50)) \nplot(hc, main = \"inhibition\")\n\n\n\n\n\n\n\nor a combination:\n\n#ff &lt;- function(x,y) { 4 * exp(2 * abs(x) - 1) }\nff &lt;- function(x,y) 10 * x\nZ &lt;- as.im(ff, owin())\nY &lt;- rMatClust(10, 0.05, Z)\nplot(Y)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#checking-homogeneity",
    "href": "day2.html#checking-homogeneity",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.5 Checking homogeneity",
    "text": "2.5 Checking homogeneity\n\n(q = quadrat.test(CSR))\n# Warning: Some expected counts are small; chi^2 approximation may be\n# inaccurate\n# \n#   Chi-squared test of CSR using quadrat counts\n# \n# data:  CSR\n# X2 = 25, df = 24, p-value = 0.9\n# alternative hypothesis: two.sided\n# \n# Quadrats: 5 by 5 grid of tiles\nplot(q)\n\n\n\n\n\n\n(q = quadrat.test(ppi))\n# \n#   Chi-squared test of CSR using quadrat counts\n# \n# data:  ppi\n# X2 = 81, df = 24, p-value = 8e-08\n# alternative hypothesis: two.sided\n# \n# Quadrats: 5 by 5 grid of tiles\nplot(q)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#estimating-density",
    "href": "day2.html#estimating-density",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.6 Estimating density",
    "text": "2.6 Estimating density\n\nmain parameter: bandwidth (sigma): determines the amound of smoothing.\nif sigma is not specified: uses bw.diggle, an automatically tuned bandwidth\n\nCorrection for edge effect?\n\ndensity(CSR) |&gt; plot()\n# Warning in seq.default(xrange[1L], xrange[2L], length = n + 1L):\n# partial argument match of 'length' to 'length.out'\nplot(CSR, add = TRUE, col = 'green')\n\n\n\n\n\n\ndensity(ppi) |&gt; plot()\n# Warning in seq.default(xrange[1L], xrange[2L], length = n + 1L):\n# partial argument match of 'length' to 'length.out'\nplot(ppi, add = TRUE, col = 'green')\n\n\n\n\n\n\ndensity(ppi, sigma = .05) |&gt; plot()\n# Warning in seq.default(xrange[1L], xrange[2L], length = n + 1L):\n# partial argument match of 'length' to 'length.out'\nplot(ppi, add = TRUE, col = 'green')",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#assessing-interactions-clusteringinhibition",
    "href": "day2.html#assessing-interactions-clusteringinhibition",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.7 Assessing interactions: clustering/inhibition",
    "text": "2.7 Assessing interactions: clustering/inhibition\nThe K-function (“Ripley’s K”) is the expected number of additional random (CSR) points within a distance r of a typical random point in the observation window.\nThe G-function (nearest neighbour distance distribution) is the cumulative distribution function G of the distance from a typical random point of X to the nearest other point of X.\n\nenvelope(CSR, Lest) |&gt; plot()\n# Generating 99 simulations of CSR  ...\n# 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n# 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n# 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n# 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n# 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n# 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n# 99.\n# \n# Done.\n\n\n\n\n\n\nenvelope(cl, Lest) |&gt; plot()\n# Generating 99 simulations of CSR  ...\n# 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n# 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n# 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n# 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n# 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n# 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n# 99.\n# \n# Done.\n\n\n\n\n\n\nenvelope(hc, Lest) |&gt; plot()\n# Generating 99 simulations of CSR  ...\n# 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n# 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n# 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n# 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n# 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n# 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n# 99.\n# \n# Done.\n\n\n\n\n\n\nenvelope(ppi, Lest) |&gt; plot()\n# Generating 99 simulations of CSR  ...\n# 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n# 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n# 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n# 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n# 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n# 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n# 99.\n# \n# Done.\n\n\n\n\n\n\nenvelope(ppi, Linhom) |&gt; plot()\n# Generating 99 simulations of CSR  ...\n# 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n# 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n# 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n# 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n# 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n# 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n# 99.\n# \n# Done.\n\n\n\n\n\n\nenvelope(Y , Lest) |&gt; plot()\n# Generating 99 simulations of CSR  ...\n# 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n# 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n# 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n# 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n# 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n# 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n# 99.\n# \n# Done.\n\n\n\n\n\n\nenvelope(Y , Linhom) |&gt; plot()\n# Generating 99 simulations of CSR  ...\n# 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,\n# 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n# 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n# 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n# 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n# 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, \n# 99.\n# \n# Done.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#fitting-models-to-clustered-data",
    "href": "day2.html#fitting-models-to-clustered-data",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.8 Fitting models to clustered data",
    "text": "2.8 Fitting models to clustered data\n\n# assuming Inhomogeneous Poisson:\nppm(ppi, ~x)\n# Warning in FIT$coef: partial match of 'coef' to 'coefficients'\n# Nonstationary Poisson process\n# Fitted to point pattern dataset 'ppi'\n# \n# Log intensity:  ~x\n# \n# Fitted trend coefficients:\n# (Intercept)           x \n#        4.33        1.96 \n# \n#             Estimate  S.E. CI95.lo CI95.hi Ztest  Zval\n# (Intercept)     4.33 0.174    3.99    4.67   *** 24.91\n# x               1.96 0.247    1.48    2.45   ***  7.96\n# assuming Inhomogeneous clustered:\nkppm(Y, ~x)\n# Warning in FIT$coef: partial match of 'coef' to 'coefficients'\n# Inhomogeneous cluster point process model\n# Fitted to point pattern dataset 'Y'\n# Fitted by minimum contrast\n#   Summary statistic: inhomogeneous K-function\n# \n# Log intensity:  ~x\n# \n# Fitted trend coefficients:\n# (Intercept)           x \n#        3.69        1.47 \n# \n# Cluster model: Thomas process\n# Fitted cluster parameters:\n# kappa scale \n# 7.731 0.038 \n# Mean cluster size:  [pixel image]\n# \n# Cluster strength: phi =  7.122\n# Sibling probability: psib =  0.8769",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#maxent-and-species-distribution-modelling",
    "href": "day2.html#maxent-and-species-distribution-modelling",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.9 MaxEnt and species distribution modelling",
    "text": "2.9 MaxEnt and species distribution modelling\nIt seems that MaxEnt fits an inhomogeneous Poisson process\nStarting from presence (only) observations, it\n\nadds background (absence) points, uniformly in space\n\nfits logistic regression models to the 0/1 data, using environmental covariates\nignores spatial interactions, spatial distances\n\nR package maxnet does that using glmnet (lasso or elasticnet regularization on)\nA maxnet example using stars is available in the development version, which can be installed directly from github by remotes::install_github(\"mrmaxent/maxnet\") ; and the same maxnet example using terra (thanks to Ben Tupper).\nRelevant papers:\n\na paper detailing the equivalence and differences between point pattern models and MaxEnt is found here.\nA statistical explanation of MaxEnt for Ecologists",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#exercises-1",
    "href": "day2.html#exercises-1",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.10 Exercises",
    "text": "2.10 Exercises\n\nFrom the point pattern shown in section 1.3, download the data as GeoPackage, and read into R\nRead the boundary of Germany using `rnaturalearth::ne_countries(scale = “larger”, country = “Germany”)\nCreate a plot showing both the observation window and the point pattern\nDo all observations fall inside the observation window?\nCreate a ppp object from the points and the window\nCreate a density map of the wind turbines, with the turbines added\nTest whether the point pattern is homogeneous\nCreate a plot with the (estimated) density of the wind turbines, with the turbine points added\nVerify that the mean density multiplied by the area of the window approximates the number of turbines\nTest for interaction: create diagnostic plots to verify whether the point pattern is clustered, or exhibits repulsion",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day2.html#further-reading",
    "href": "day2.html#further-reading",
    "title": "\n2  Point Pattern data\n",
    "section": "\n2.11 Further reading",
    "text": "2.11 Further reading\n\nE. Pebesma, 2018. Simple Features for R: Standardized Support for Spatial Vector Data. The R Journal 10:1, 439-446.\nA. Baddeley, E. Rubak and R Turner, 2016. Spatial Point Patterns: methodology and Applications in R; Chapman and Hall/CRC 810 pages.\nJ. Illian, A. Penttinen, H. Stoyan and D. Stoyan, 2008. Statistical Analysis and Modelling of Spatial Point Patterns; Wiley, 534 pages.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Point Pattern data</span>"
    ]
  },
  {
    "objectID": "day3.html",
    "href": "day3.html",
    "title": "\n3  Geostatistical data\n",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#gstat",
    "href": "day3.html#gstat",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.1 gstat\n",
    "text": "3.1 gstat\n\nR package gstat was written in 2002/3, from a stand-alone C program that was released under the GPL in 1997. It implements “basic” geostatistical functions for modelling spatial dependence (variograms), kriging interpolation and conditional simulation. It can be used for multivariable kriging (cokriging), as well as for spatiotemporal variography and kriging. Recent updates included support for sf and stars objects.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#what-are-geostatistical-data",
    "href": "day3.html#what-are-geostatistical-data",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.2 What are geostatistical data?",
    "text": "3.2 What are geostatistical data?\nRecall from day 1: locations + measured values\n\nThe value of interest is measured at a set of sample locations\nAt other location, this value exists but is missing\n\nThe interest is in estimating (predicting) this missing value (interpolation)\nThe actual sample locations are not of (primary) interest, the signal is in the measured values\n\n\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\nno2 &lt;- read.csv(system.file(\"external/no2.csv\",\n    package = \"gstat\"))\ncrs &lt;- st_crs(\"EPSG:32632\") # a csv doesn't carry a CRS!\nst_as_sf(no2, crs = \"OGC:CRS84\", coords =\n    c(\"station_longitude_deg\", \"station_latitude_deg\")) |&gt;\n    st_transform(crs) -&gt; no2.sf\nlibrary(ggplot2)\n# plot(st_geometry(no2.sf))\n\"https://github.com/edzer/sdsr/raw/main/data/de_nuts1.gpkg\" |&gt;\n  read_sf() |&gt;\n  st_transform(crs) -&gt; de\nggplot() + geom_sf(data = de) +\n    geom_sf(data = no2.sf, mapping = aes(col = NO2))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#spatial-correlation",
    "href": "day3.html#spatial-correlation",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.3 Spatial correlation",
    "text": "3.3 Spatial correlation\nLagged scatterplots\n“by hand”, base R:\n\n(w = st_is_within_distance(no2.sf, no2.sf, units::set_units(50, km), \n                          retain_unique = TRUE))\n# Sparse geometry binary predicate list of length 74, where\n# the predicate was `is_within_distance', with retain_unique =\n# TRUE\n# first 10 elements:\n#  1: (empty)\n#  2: (empty)\n#  3: 4, 5, 26\n#  4: 5, 26\n#  5: (empty)\n#  6: 30, 72\n#  7: (empty)\n#  8: (empty)\n#  9: (empty)\n#  10: (empty)\nd = as.data.frame(w)\nx = no2.sf$NO2[d$row.id]\ny = no2.sf$NO2[d$col.id]\ncor(x, y)\n# [1] 0.296\nplot(x, y, main = \"lagged scatterplot\")\nabline(0, 1)\n\n\n\n\n\n\n\nusing gstat:\n\nlibrary(gstat)\nhscat(NO2~1, no2.sf, breaks = c(0,50,100,150,200,250)*1000)\n\n\n\n\n\n\n\nVariogram\nWhen we assume \\(Z(s)\\) has a constant and unknown mean, the spatial dependence can be described by the variogram, defined as \\(\\gamma(h)\n= 0.5 E(Z(s)-Z(s+h))^2\\). If the random process \\(Z(s)\\) has a finite variance, then the variogram is related to the covariance function \\(C(h)\\) by \\(\\gamma(h) = C(0)-C(h)\\).\nThe variogram can be estimated from sample data by averaging squared differences: \\[\\hat{\\gamma}(\\tilde{h})=\\frac{1}{2N_h}\\sum_{i=1}^{N_h}(Z(s_i)-Z(s_i+h))^2 \\ \\\nh \\in \\tilde{h}\\]\n\ndivide by \\(2N_h\\):\n\nif finite, \\(\\gamma(\\infty)=\\sigma^2=C(0)\\)\n\n\nsemi variance\n\n\nif data are not gridded, group \\(N_h\\) pairs \\(s_i,s_i+h\\) for which \\(h \\in \\tilde{h}\\), \\(\\tilde{h}=[h_1,h_2]\\)\n\nrule-of-thumb: choose about 10-25 distance intervals \\(\\tilde{h}\\), from length 0 to about on third of the area size\nplot \\(\\gamma\\) against \\(\\tilde{h}\\) taken as the average value of all \\(h \\in \\tilde{h}\\)\n\n\nWe can compute a variogram “by hand”, using base R:\n\nz = no2.sf$NO2\nz2 = 0.5 * outer(z, z, FUN = \"-\")^2 # (Z(s)-Z(s+h))^2\nd = as.matrix(st_distance(no2.sf))  # h\nvcloud = data.frame(dist = as.vector(d), gamma = as.vector(z2))\nvcloud = vcloud[vcloud$dist != 0,]\nvcloud$dclass = cut(vcloud$dist, c(0, 50, 100, 150, 200, 250, 300, 350) * 1000)\nv = aggregate(gamma~dclass, vcloud, mean)\nplot(gamma ~ dclass, v, ylim = c(0, 20))\n\n\n\n\n\n\n\nusing gstat:\n\nvv = variogram(NO2~1, no2.sf, width = 50000, cutoff = 350000)\nvv$gamma - v$gamma\n# [1]  3.55e-15 -3.55e-15  5.33e-15 -5.33e-15  7.11e-15 -1.42e-14\n# [7]  5.33e-15\nplot(vv)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreakout session 1\n\n\n\nCompute the variogram of NO2 using argument cloud = TRUE.\n\nhow does the resulting object differ from the “regular” variogram\nwhat do the “left” and “right” fields refer to?\nwhen we plot the resulting object, does it still indicate spatial correlation?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#interpolation",
    "href": "day3.html#interpolation",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.4 Interpolation",
    "text": "3.4 Interpolation\nFor interpolation, we first need a target grid (point patterns have an observation window, geostatistical data not!)\nA simple interpolator (that is hard to beat) is the inverse distance interpolator, \\[\\hat{Z}(s_0) = \\sum_{j=1}^n \\lambda_j Z(s_i)\\] with \\(\\lambda_j\\) proportional to \\(||s_i - s_0||^{-p}\\) and normalized to sum to one (weighted mean), and \\(p\\) tunable but defaulting to 2.\nUsing the data range:\n\nlibrary(stars)\n# Loading required package: abind\ng1 = st_as_stars(st_bbox(no2.sf))\nlibrary(gstat)\nidw(NO2~1, no2.sf, g1) |&gt; plot(reset = FALSE)\n# [inverse distance weighted interpolation]\nplot(st_geometry(no2.sf), add = TRUE, col = 'yellow')\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'red')\n\n\n\n\n\n\n\nBetter to use the outer polygon:\n\ng2 = st_as_stars(st_bbox(de))\nidw(NO2~1, no2.sf, g2) |&gt; plot(reset = FALSE)\n# [inverse distance weighted interpolation]\nplot(st_geometry(no2.sf), add = TRUE, col = 'yellow')\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'red')\n\n\n\n\n\n\n\nAnd crop to (mask out outside) the area of interest:\n\ng3 = st_crop(g2, de)\ni = idw(NO2~1, no2.sf, g3) \n# [inverse distance weighted interpolation]\nplot(i, reset = FALSE, main = \"yearly mean NO2, rural background\")\nplot(st_geometry(no2.sf), add = TRUE, col = 'yellow')\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'red')\n\n\n\n\n\n\n\nGeostatistical approaches compute weights based on covariances between observations \\(Z(s_i)\\), and between observations and the value at the interpolation location \\(Z(s_0)\\). These covariances are obtained from a model fitted to the sample variogram.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#fit-a-variogram-model",
    "href": "day3.html#fit-a-variogram-model",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.5 Fit a variogram model",
    "text": "3.5 Fit a variogram model\n\n# The sample variogram:\nv = variogram(NO2~1, no2.sf)\nplot(v)\n\n\n\n\n\n\n\nfit a model, e.g. an exponential model:\n\nv.fit = fit.variogram(v, vgm(1, \"Exp\", 50000))\nplot(v, v.fit)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#blupkriging",
    "href": "day3.html#blupkriging",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.6 BLUP/Kriging",
    "text": "3.6 BLUP/Kriging\nGiven this model, we can interpolate using the best unbiased linear predictor (BLUP), also called kriging predictor. Under the model \\(Z(s)=m+e(s)\\) it estimates \\(m\\) using generalized least squares, and predicts \\(e(s)\\) using a weighted mean, where weights are chosen such that \\(Var(Z(s_0)-\\hat{Z}(s_0))\\) is minimized.\n\nk = krige(NO2~1, no2.sf, g3, v.fit)\n# [using ordinary kriging]\nk$idw = i$var1.pred\nk$kriging = k$var1.pred\nhook = function() {\n  plot(st_geometry(no2.sf), add = TRUE, col = 'yellow')\n  plot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'red')\n}\nplot(merge(k[c(\"kriging\", \"idw\")]), hook = hook, breaks = \"equal\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nDensity or interpolation?\n\n\n\nBoth density maps shown in the Point Pattern section and interpolated maps shown in this section look very similar:\n\nraster maps with continuous values\nsmooth spatial patterns\n\nThe differences could not be larger!\n\npoint density estimates estimate the number of points per unit area; the values are (normalized) counts\n\ninterpolated maps estimate an unmeasured continuous variable; the values are weighted averages of an attribute\n\n\n\n\nTo illustrate the difference between density and interpolated values:\n\n# Loading required package: spatstat.data\n# Loading required package: spatstat.univar\n# spatstat.univar 3.0-1\n# Loading required package: spatstat.geom\n# spatstat.geom 3.3-3\n# Loading required package: spatstat.random\n# spatstat.random 3.3-2\n# Loading required package: spatstat.explore\n# Loading required package: nlme\n# spatstat.explore 3.3-2\n# \n# Attaching package: 'spatstat.explore'\n# The following object is masked from 'package:gstat':\n# \n#     idw\n# Loading required package: spatstat.model\n# Loading required package: rpart\n# spatstat.model 3.3-2\n# Loading required package: spatstat.linnet\n# spatstat.linnet 3.2-2\n# \n# spatstat 3.2-1 \n# For an introduction to spatstat, type 'beginner'",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#kriging-with-a-non-constant-mean",
    "href": "day3.html#kriging-with-a-non-constant-mean",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.7 Kriging with a non-constant mean",
    "text": "3.7 Kriging with a non-constant mean\nUnder the model \\(Z(s) = X(s)\\beta + e(s)\\), \\(\\beta\\) is estimated using generalized least squares, and the variogram of regression residuals is needed; see Ch 12.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#conditional-simulation",
    "href": "day3.html#conditional-simulation",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.8 Conditional simulation",
    "text": "3.8 Conditional simulation\nSimulating spatially correlated data\nUsing a coarse grid, with base R:\n\nset.seed(13579)\ng2c = st_as_stars(st_bbox(de), dx = 15000)\ng3c = st_crop(g2c, de)\np = st_as_sf(g3c, as_points = TRUE)\nd = st_distance(p)\nSigma = variogramLine(v.fit, covariance = TRUE, dist_vector = d)\nn = 100\nch = chol(Sigma)\nsim = matrix(rnorm(n * nrow(ch)), nrow = n) %*% ch + mean(no2.sf$NO2)\nfor (i in seq_len(n)) {\n    m = g3c[[1]]\n    m[!is.na(m)] = sim[i,]\n    g3c[[ paste0(\"sim\", i) ]] = m\n}\nplot(merge(g3c[2:11]), breaks = \"equal\")\n\n\n\n\n\n\n\nAs a check, we could compute the variogram of some of the realisations:\n\ng3c[\"sim4\"] |&gt; \n  st_as_sf() |&gt; \n  variogram(sim4~1, data = _) |&gt; \n  plot(model = v.fit)\n\n\n\n\n\n\ng3c[\"sim5\"] |&gt; \n  st_as_sf() |&gt; \n  variogram(sim5~1, data = _) |&gt; \n  plot(model = v.fit, ylim = c(0,17.5))\n\n\n\n\n\n\ng3c[\"sim6\"] |&gt; \n  st_as_sf() |&gt; \n  variogram(sim6~1, data = _) |&gt; \n  plot(model = v.fit, ylim = c(0,17.5))\n\n\n\n\n\n\n\nThe mean of these simulations is constant, not related to measured values:\n\nst_apply(merge(g3c[-1]), c(\"x\", \"y\"), mean) |&gt; plot()\n\n\n\n\n\n\nmean(no2.sf$NO2)\n# [1] 8.38\n\nConditioning simulations on measured values can be done with gstat, using conditional simulation\n\ncs = krige(NO2~1, no2.sf, g3, v.fit, nsim = 50, nmax = 30)\n# drawing 50 GLS realisations of beta...\n# [using conditional Gaussian simulation]\nplot(cs[,,,1:10])\n\n\n\n\n\n\n\nWe see that these simulations are much more alike; also their mean and variance resemble that of the kriging mean and variance:\n\ncsm = st_apply(cs, c(\"x\", \"y\"), mean)\ncsm$kriging = krige(NO2~1, no2.sf, g3, v.fit)[1]\n# [using ordinary kriging]\nplot(merge(csm), breaks = \"equal\")\n\n\n\n\n\n\ncsv = st_apply(cs, c(\"x\", \"y\"), var)\ncsv$kr_var = krige(NO2~1, no2.sf, g3, v.fit)[2]\n# [using ordinary kriging]\nplot(merge(csv), breaks = \"equal\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreakout session 2\n\n\n\nWhat causes the differences between the mean and the variance of the simulations (left) and the mean and variance obtained by kriging (right)?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#spatiotemporal-geostatistics",
    "href": "day3.html#spatiotemporal-geostatistics",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.9 SpatioTemporal geostatistics",
    "text": "3.9 SpatioTemporal geostatistics\n\nCh 13",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day3.html#further-reading",
    "href": "day3.html#further-reading",
    "title": "\n3  Geostatistical data\n",
    "section": "\n3.10 Further reading",
    "text": "3.10 Further reading\n\nPebesma, E.J., 2004. Multivariable geostatistics in S: the gstat package. Computers & Geosciences, 30: 683-691.\nBenedikt Gräler, Edzer Pebesma and Gerard Heuvelink, 2016. Spatio-Temporal Interpolation using gstat. The R Journal 8(1), 204-218",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Geostatistical data</span>"
    ]
  },
  {
    "objectID": "day4.html",
    "href": "day4.html",
    "title": "\n4  Machine Learning methods applied to spatial data\n",
    "section": "",
    "text": "Learning goals",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning methods applied to spatial data</span>"
    ]
  },
  {
    "objectID": "day4.html#spatial-coordinates-as-predictor",
    "href": "day4.html#spatial-coordinates-as-predictor",
    "title": "\n4  Machine Learning methods applied to spatial data\n",
    "section": "\n4.1 Spatial coordinates as predictor",
    "text": "4.1 Spatial coordinates as predictor\nWe’ll rename coordinates to x and y:\n\nlibrary(dplyr)\n# \n# Attaching package: 'dplyr'\n# The following objects are masked from 'package:stats':\n# \n#     filter, lag\n# The following objects are masked from 'package:base':\n# \n#     intersect, setdiff, setequal, union\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\ncrs &lt;- st_crs(\"EPSG:32632\") # a csv doesn't carry a CRS!\nno2 &lt;- read.csv(system.file(\"external/no2.csv\",\n    package = \"gstat\")) \nno2 |&gt; rename(x = station_longitude_deg, y = station_latitude_deg)  |&gt; \n  st_as_sf(crs = \"OGC:CRS84\", coords =\n    c(\"x\", \"y\"), remove = FALSE) |&gt;\n    st_transform(crs) -&gt; no2.sf\n# we need to reassign x and y:\ncc = st_coordinates(no2.sf)\nno2.sf$x = cc[,1]\nno2.sf$y = cc[,2]\nhead(no2.sf)\n# Simple feature collection with 6 features and 21 fields\n# Geometry type: POINT\n# Dimension:     XY\n# Bounding box:  xmin: 495000 ymin: 5320000 xmax: 816000 ymax: 5930000\n# Projected CRS: WGS 84 / UTM zone 32N\n#   station_european_code station_local_code country_iso_code\n# 1               DENI063            DENI063               DE\n# 2               DEBY109            DEBY109               DE\n# 3               DEBE056            DEBE056               DE\n# 4               DEBE062            DEBE062               DE\n# 5               DEBE032            DEBE032               DE\n# 6               DEHE046            DEHE046               DE\n#   country_name                station_name station_start_date\n# 1      Germany                  Altes Land         1999-02-11\n# 2      Germany          Andechs/Rothenfeld         2003-04-17\n# 3      Germany           B Friedrichshagen         1994-02-01\n# 4      Germany B Frohnau, Funkturm (3.5 m)         1996-02-01\n# 5      Germany         B Grunewald (3.5 m)         1986-10-01\n# 6      Germany                 Bad Arolsen         1999-05-11\n#   station_end_date type_of_station station_ozone_classification\n# 1               NA      Background                        rural\n# 2               NA      Background                        rural\n# 3               NA      Background                        rural\n# 4               NA      Background                        rural\n# 5               NA      Background                        rural\n# 6               NA      Background                        rural\n#   station_type_of_area station_subcat_rural_back street_type      x\n# 1                rural                   unknown             545414\n# 2                rural                  regional             665711\n# 3                rural                 near city             815741\n# 4                rural                 near city             790544\n# 5                rural                 near city             786923\n# 6                rural                   unknown             495007\n#         y station_altitude          station_city lau_level1_code\n# 1 5930802                3                                    NA\n# 2 5315213              700                                    NA\n# 3 5820995               35                                    NA\n# 4 5842367               50                BERLIN              NA\n# 5 5822067               50                BERLIN              NA\n# 6 5697747              343 BAD AROLSEN/KOHLGRUND              NA\n#   lau_level2_code    lau_level2_name EMEP_station   NO2\n# 1         3359028               Jork           no 13.10\n# 2         9188117            Andechs           no  7.14\n# 3        11000000      Berlin, Stadt           no 12.80\n# 4        11000000      Berlin, Stadt           no 11.83\n# 5        11000000      Berlin, Stadt           no 11.98\n# 6         6635002 Bad Arolsen, Stadt           no  8.94\n#                 geometry\n# 1 POINT (545414 5930802)\n# 2 POINT (665711 5315213)\n# 3 POINT (815741 5820995)\n# 4 POINT (790544 5842367)\n# 5 POINT (786923 5822067)\n# 6 POINT (495007 5697747)\n\"https://github.com/edzer/sdsr/raw/main/data/de_nuts1.gpkg\" |&gt;\n  read_sf() |&gt;\n  st_transform(crs) -&gt; de\n\n\nlibrary(stars)\n# Loading required package: abind\ng2 = st_as_stars(st_bbox(de))\ng3 = st_crop(g2, de)\ng4 = st_rasterize(de, g3)\ng4$ID_1[g4$ID_1 == 758] = NA\ng4$ID1 = as.factor(g4$ID_1) # now a factor:\nplot(g4[\"ID1\"], reset = FALSE)\nplot(st_geometry(no2.sf), add = TRUE, col = 'green')\n\n\n\n\n\n\nno2.sf$ID1 = st_extract(g4, no2.sf)$ID1\nno2.sf$ID1 |&gt; summary()\n#  753  754  755  756  759  760  761  762  763  764  765  766  767 \n#    4    8    3    4   10    6    6    6    6    1    6    5    1 \n#  768 NA's \n#    6    2\n\nSimple ANOVA type predictor:\n\nlm1 = lm(NO2~ID1, no2.sf)\nsummary(lm1)\n# \n# Call:\n# lm(formula = NO2 ~ ID1, data = no2.sf)\n# \n# Residuals:\n#    Min     1Q Median     3Q    Max \n# -6.324 -1.599 -0.311  0.859 12.358 \n# \n# Coefficients:\n#             Estimate Std. Error t value Pr(&gt;|t|)    \n# (Intercept)    8.136      1.873    4.34  5.7e-05 ***\n# ID1754         1.883      2.294    0.82     0.41    \n# ID1755         4.068      2.861    1.42     0.16    \n# ID1756        -1.593      2.649   -0.60     0.55    \n# ID1759         1.015      2.216    0.46     0.65    \n# ID1760        -2.215      2.418   -0.92     0.36    \n# ID1761         1.353      2.418    0.56     0.58    \n# ID1762         3.697      2.418    1.53     0.13    \n# ID1763        -1.724      2.418   -0.71     0.48    \n# ID1764         1.091      4.188    0.26     0.80    \n# ID1765         0.591      2.418    0.24     0.81    \n# ID1766        -2.282      2.513   -0.91     0.37    \n# ID1767         1.024      4.188    0.24     0.81    \n# ID1768        -2.358      2.418   -0.98     0.33    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 3.75 on 58 degrees of freedom\n#   (2 observations deleted due to missingness)\n# Multiple R-squared:  0.268,   Adjusted R-squared:  0.104 \n# F-statistic: 1.63 on 13 and 58 DF,  p-value: 0.102\ng4$NO2_aov = predict(lm1, as.data.frame(g4))\nplot(g4[\"NO2_aov\"], breaks = \"equal\", reset = FALSE)\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\nSimple linear models in coordinates: trend surfaces\n\nlm2 = lm(NO2~x+y, no2.sf)\nsummary(lm2)\n# \n# Call:\n# lm(formula = NO2 ~ x + y, data = no2.sf)\n# \n# Residuals:\n#    Min     1Q Median     3Q    Max \n# -6.880 -2.634 -0.991  1.431 11.660 \n# \n# Coefficients:\n#              Estimate Std. Error t value Pr(&gt;|t|)\n# (Intercept)  9.47e+00   1.36e+01    0.70     0.49\n# x           -3.66e-06   3.00e-06   -1.22     0.23\n# y            1.91e-07   2.45e-06    0.08     0.94\n# \n# Residual standard error: 3.95 on 71 degrees of freedom\n# Multiple R-squared:  0.0212,  Adjusted R-squared:  -0.00637 \n# F-statistic: 0.769 on 2 and 71 DF,  p-value: 0.467\ncc = st_coordinates(g4)\ng4$x = cc[,1]\ng4$y = cc[,2]\ng4$NO2_lm2 = predict(lm2, g4)\nplot(g4[\"NO2_lm2\"], breaks = \"equal\", reset = FALSE, main = \"1st order polynomial\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\n\nlm3 = lm(NO2~x+y+I(x^2)+I(y^2)+I(x*y), no2.sf)\nsummary(lm3)\n# \n# Call:\n# lm(formula = NO2 ~ x + y + I(x^2) + I(y^2) + I(x * y), data = no2.sf)\n# \n# Residuals:\n#    Min     1Q Median     3Q    Max \n# -5.480 -2.583 -0.585  1.523 12.750 \n# \n# Coefficients:\n#              Estimate Std. Error t value Pr(&gt;|t|)  \n# (Intercept) -4.24e+02   3.27e+02   -1.30    0.199  \n# x            1.34e-04   9.13e-05    1.47    0.147  \n# y            1.39e-04   1.14e-04    1.21    0.230  \n# I(x^2)       2.52e-11   1.91e-11    1.32    0.190  \n# I(y^2)      -1.06e-11   1.01e-11   -1.05    0.296  \n# I(x * y)    -2.96e-11   1.65e-11   -1.79    0.077 .\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 3.87 on 68 degrees of freedom\n# Multiple R-squared:  0.0972,  Adjusted R-squared:  0.0308 \n# F-statistic: 1.46 on 5 and 68 DF,  p-value: 0.213\ng4$NO2_lm3 = predict(lm3, g4)\nplot(g4[\"NO2_lm3\"], breaks = \"equal\", reset = FALSE, main = \"2nd order polynomial\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\n\nlm4 = lm(NO2~x+y+I(x^2)+I(y^2)+I(x*y)+I(x^3)+I(x^2*y)+I(x*y^2)+I(y^3), no2.sf)\nsummary(lm4)\n# \n# Call:\n# lm(formula = NO2 ~ x + y + I(x^2) + I(y^2) + I(x * y) + I(x^3) + \n#     I(x^2 * y) + I(x * y^2) + I(y^3), data = no2.sf)\n# \n# Residuals:\n#    Min     1Q Median     3Q    Max \n# -5.285 -2.582 -0.796  2.074 12.693 \n# \n# Coefficients:\n#              Estimate Std. Error t value Pr(&gt;|t|)  \n# (Intercept)  3.38e+03   9.12e+03    0.37    0.712  \n# x            5.15e-03   2.50e-03    2.06    0.043 *\n# y           -2.40e-03   4.84e-03   -0.49    0.622  \n# I(x^2)      -1.19e-09   7.46e-10   -1.60    0.115  \n# I(y^2)       5.15e-10   8.59e-10    0.60    0.551  \n# I(x * y)    -1.54e-09   8.57e-10   -1.80    0.077 .\n# I(x^3)       1.13e-16   1.31e-16    0.86    0.394  \n# I(x^2 * y)   1.80e-16   1.38e-16    1.30    0.197  \n# I(x * y^2)   1.14e-16   7.75e-17    1.47    0.146  \n# I(y^3)      -3.49e-17   5.10e-17   -0.68    0.496  \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 3.82 on 64 degrees of freedom\n# Multiple R-squared:  0.173,   Adjusted R-squared:  0.0572 \n# F-statistic: 1.49 on 9 and 64 DF,  p-value: 0.17\ng4$NO2_lm4 = predict(lm4, g4)\nplot(g4[\"NO2_lm4\"], breaks = \"equal\", reset = FALSE, main = \"3rd order polynomial\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreakout session 1\n\n\n\nDiscuss:\n\nhow will the predicted surface look like when instead of (functions of) coordinates, the variable elevation is used (e.g. to predict average temperatures)?\nwhat will be the value range, approximately, of the resulting predicted values?\n\n\n\nregression tree\n\nlibrary(rpart)\ntree = rpart(NO2~., as.data.frame(no2.sf)[c(\"NO2\", \"x\", \"y\")])\ng4$tree = predict(tree, as.data.frame(g4))\nplot(g4[\"tree\"], breaks = \"equal\", reset = FALSE, main = \"regression tree\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\nRandom forest\n\nlibrary(randomForest)\n# randomForest 4.7-1.2\n# Type rfNews() to see new features/changes/bug fixes.\n# \n# Attaching package: 'randomForest'\n# The following object is masked from 'package:dplyr':\n# \n#     combine\nrf = randomForest(NO2~., as.data.frame(no2.sf)[c(\"NO2\", \"x\", \"y\")])\n# Warning in seq.default(along = m): partial argument match of\n# 'along' to 'along.with'\ng4$rf = predict(rf, as.data.frame(g4))\nplot(g4[\"rf\"], breaks = \"equal\", reset = FALSE, main = \"random forest\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\nRotated coordinates:\n\nlibrary(randomForest)\nno2.sf$x1 = no2.sf$x + no2.sf$y\nno2.sf$y1 = no2.sf$x - no2.sf$y\nrf = randomForest(NO2~., as.data.frame(no2.sf)[c(\"NO2\", \"x1\", \"y1\")])\n# Warning in seq.default(along = m): partial argument match of\n# 'along' to 'along.with'\ng4$x1 = g4$x + g4$y\ng4$y1 = g4$x - g4$y\ng4$rf_rot = predict(rf, as.data.frame(g4))\nplot(g4[\"rf_rot\"], breaks = \"equal\", reset = FALSE, main = \"random forest\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\nUsing distance variables:\n\nst_bbox(de) |&gt; st_as_sfc() |&gt; st_cast(\"POINT\") -&gt; pts\npts = c(pts[1:4], st_centroid(st_geometry(de)))\nd = st_distance(st_as_sfc(g4, as_points = TRUE), pts)\nfor (i in seq_len(ncol(d))) {\n    g4[[ paste0(\"d_\", i) ]] = d[,i]\n}\ne = st_extract(g4, no2.sf)\nfor (i in seq_len(ncol(d))) {\n    no2.sf[[ paste0(\"d_\", i) ]] = e[[16+i]]\n}\n(n = names(g4))\n#  [1] \"ID_0\"       \"ID_1\"       \"Shape_Leng\" \"Shape_Area\"\n#  [5] \"ID1\"        \"NO2_aov\"    \"x\"          \"y\"         \n#  [9] \"NO2_lm2\"    \"NO2_lm3\"    \"NO2_lm4\"    \"tree\"      \n# [13] \"rf\"         \"x1\"         \"y1\"         \"rf_rot\"    \n# [17] \"d_1\"        \"d_2\"        \"d_3\"        \"d_4\"       \n# [21] \"d_5\"        \"d_6\"        \"d_7\"        \"d_8\"       \n# [25] \"d_9\"        \"d_10\"       \"d_11\"       \"d_12\"      \n# [29] \"d_13\"       \"d_14\"       \"d_15\"       \"d_16\"      \n# [33] \"d_17\"       \"d_18\"       \"d_19\"       \"d_20\"\nplot(merge(g4[grepl(\"d_\", n)]))\n\n\n\n\n\n\n\n\nlibrary(randomForest)\nrf = randomForest(NO2~., as.data.frame(no2.sf)[c(\"NO2\", n[grepl(\"d_\", n)])])\n# Warning in seq.default(along = m): partial argument match of\n# 'along' to 'along.with'\ng4$rf_d = predict(rf, as.data.frame(g4))\nplot(g4[\"rf_d\"], breaks = \"equal\", reset = FALSE, main = \"random forest\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\nAdding more…\n\npts = st_sample(de, 200, type = \"regular\")\nd = st_distance(st_as_sfc(g4, as_points = TRUE), pts)\nfor (i in seq_len(ncol(d))) {\n    g4[[ paste0(\"d_\", i) ]] = d[,i]\n}\ne = st_extract(g4, no2.sf)\nfor (i in seq_len(ncol(d))) {\n    no2.sf[[ paste0(\"d_\", i) ]] = e[[16+i]]\n}\n(n = names(g4))\n#   [1] \"ID_0\"       \"ID_1\"       \"Shape_Leng\" \"Shape_Area\"\n#   [5] \"ID1\"        \"NO2_aov\"    \"x\"          \"y\"         \n#   [9] \"NO2_lm2\"    \"NO2_lm3\"    \"NO2_lm4\"    \"tree\"      \n#  [13] \"rf\"         \"x1\"         \"y1\"         \"rf_rot\"    \n#  [17] \"d_1\"        \"d_2\"        \"d_3\"        \"d_4\"       \n#  [21] \"d_5\"        \"d_6\"        \"d_7\"        \"d_8\"       \n#  [25] \"d_9\"        \"d_10\"       \"d_11\"       \"d_12\"      \n#  [29] \"d_13\"       \"d_14\"       \"d_15\"       \"d_16\"      \n#  [33] \"d_17\"       \"d_18\"       \"d_19\"       \"d_20\"      \n#  [37] \"rf_d\"       \"d_21\"       \"d_22\"       \"d_23\"      \n#  [41] \"d_24\"       \"d_25\"       \"d_26\"       \"d_27\"      \n#  [45] \"d_28\"       \"d_29\"       \"d_30\"       \"d_31\"      \n#  [49] \"d_32\"       \"d_33\"       \"d_34\"       \"d_35\"      \n#  [53] \"d_36\"       \"d_37\"       \"d_38\"       \"d_39\"      \n#  [57] \"d_40\"       \"d_41\"       \"d_42\"       \"d_43\"      \n#  [61] \"d_44\"       \"d_45\"       \"d_46\"       \"d_47\"      \n#  [65] \"d_48\"       \"d_49\"       \"d_50\"       \"d_51\"      \n#  [69] \"d_52\"       \"d_53\"       \"d_54\"       \"d_55\"      \n#  [73] \"d_56\"       \"d_57\"       \"d_58\"       \"d_59\"      \n#  [77] \"d_60\"       \"d_61\"       \"d_62\"       \"d_63\"      \n#  [81] \"d_64\"       \"d_65\"       \"d_66\"       \"d_67\"      \n#  [85] \"d_68\"       \"d_69\"       \"d_70\"       \"d_71\"      \n#  [89] \"d_72\"       \"d_73\"       \"d_74\"       \"d_75\"      \n#  [93] \"d_76\"       \"d_77\"       \"d_78\"       \"d_79\"      \n#  [97] \"d_80\"       \"d_81\"       \"d_82\"       \"d_83\"      \n# [101] \"d_84\"       \"d_85\"       \"d_86\"       \"d_87\"      \n# [105] \"d_88\"       \"d_89\"       \"d_90\"       \"d_91\"      \n# [109] \"d_92\"       \"d_93\"       \"d_94\"       \"d_95\"      \n# [113] \"d_96\"       \"d_97\"       \"d_98\"       \"d_99\"      \n# [117] \"d_100\"      \"d_101\"      \"d_102\"      \"d_103\"     \n# [121] \"d_104\"      \"d_105\"      \"d_106\"      \"d_107\"     \n# [125] \"d_108\"      \"d_109\"      \"d_110\"      \"d_111\"     \n# [129] \"d_112\"      \"d_113\"      \"d_114\"      \"d_115\"     \n# [133] \"d_116\"      \"d_117\"      \"d_118\"      \"d_119\"     \n# [137] \"d_120\"      \"d_121\"      \"d_122\"      \"d_123\"     \n# [141] \"d_124\"      \"d_125\"      \"d_126\"      \"d_127\"     \n# [145] \"d_128\"      \"d_129\"      \"d_130\"      \"d_131\"     \n# [149] \"d_132\"      \"d_133\"      \"d_134\"      \"d_135\"     \n# [153] \"d_136\"      \"d_137\"      \"d_138\"      \"d_139\"     \n# [157] \"d_140\"      \"d_141\"      \"d_142\"      \"d_143\"     \n# [161] \"d_144\"      \"d_145\"      \"d_146\"      \"d_147\"     \n# [165] \"d_148\"      \"d_149\"      \"d_150\"      \"d_151\"     \n# [169] \"d_152\"      \"d_153\"      \"d_154\"      \"d_155\"     \n# [173] \"d_156\"      \"d_157\"      \"d_158\"      \"d_159\"     \n# [177] \"d_160\"      \"d_161\"      \"d_162\"      \"d_163\"     \n# [181] \"d_164\"      \"d_165\"      \"d_166\"      \"d_167\"     \n# [185] \"d_168\"      \"d_169\"      \"d_170\"      \"d_171\"     \n# [189] \"d_172\"      \"d_173\"      \"d_174\"      \"d_175\"     \n# [193] \"d_176\"      \"d_177\"      \"d_178\"      \"d_179\"     \n# [197] \"d_180\"      \"d_181\"      \"d_182\"      \"d_183\"     \n# [201] \"d_184\"      \"d_185\"      \"d_186\"      \"d_187\"     \n# [205] \"d_188\"      \"d_189\"      \"d_190\"      \"d_191\"     \n# [209] \"d_192\"      \"d_193\"      \"d_194\"      \"d_195\"     \n# [213] \"d_196\"      \"d_197\"      \"d_198\"      \"d_199\"     \n# [217] \"d_200\"\nrf = randomForest(NO2~., as.data.frame(no2.sf)[c(\"NO2\", n[grepl(\"d_\", n)])])\n# Warning in seq.default(along = m): partial argument match of\n# 'along' to 'along.with'\ng4$rf_dm = predict(rf, as.data.frame(g4))\nplot(g4[\"rf_dm\"], breaks = \"equal\", reset = FALSE, main = \"random forest\")\nplot(st_cast(st_geometry(de), \"MULTILINESTRING\"), add = TRUE, col = 'green')\n\n\n\n\n\n\n\nFurther approaches:\n\nuse linear regression on Gaussian kernel basis functions, \\(\\exp(-h^2)\\)\n\nuse splines in \\(x\\) and \\(y\\), with a given degree of smoothing (or effective degrees of freedom)\nuse additional, non-distance/coordinate functions as base function(s)\n\nprovided they are available “everywhere” (as coverage)\nexamples: elevation, bioclimatic variables, (values derived from) satellite imagery bands\n\n\n\n\n\n\n\n\n\nBreakout session 2\n\n\n\nDiscuss:\n\nHow would you assess whether residuals from your fitted model are spatially correlated?\nDoes resampling using random partitioning (as is done in random forest) implicitly assume that observations are independent?\n\n\n\nExample from CAST / caret\n\nlibrary(CAST)\nlibrary(caret)\n# Loading required package: ggplot2\n# \n# Attaching package: 'ggplot2'\n# The following object is masked from 'package:randomForest':\n# \n#     margin\n# Loading required package: lattice\ndata(splotdata)\nclass(splotdata)\n# [1] \"sf\"         \"data.frame\"\nr = read_stars(system.file(\"extdata/predictors_chile.tif\", \n                           package = \"CAST\"))\nx = st_drop_geometry(splotdata)[,6:16]\ny = splotdata$Species_richness\ntr = train(x, y) # chooses a random forest by default\n# Warning in seq.default(along = pkg): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = models$library): partial argument\n# match of 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = data): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = x): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = outcome): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = x): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(2, to = p, length = len): partial argument\n# match of 'length' to 'length.out'\n# Warning in seq.default(along = resampleIndex): partial argument\n# match of 'along' to 'along.with'\n# Warning in tmp$resample: partial match of 'resample' to 'resamples'\n# Warning in seq.default(along = paramNames): partial argument match\n# of 'along' to 'along.with'\n# Warning in seq.default(along = y): partial argument match of\n# 'along' to 'along.with'\npredict(split(r), tr) |&gt; plot()\n\n\n\n\n\n\n\nClustered data?\n\nplot(r[,,,1], reset = FALSE)\nplot(st_geometry(splotdata), add = TRUE, col = 'green')",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning methods applied to spatial data</span>"
    ]
  },
  {
    "objectID": "day4.html#cross-validation-random-or-spatially-blocked",
    "href": "day4.html#cross-validation-random-or-spatially-blocked",
    "title": "\n4  Machine Learning methods applied to spatial data\n",
    "section": "\n4.2 Cross validation: random or spatially blocked?",
    "text": "4.2 Cross validation: random or spatially blocked?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning methods applied to spatial data</span>"
    ]
  },
  {
    "objectID": "day4.html#transferrability-of-models-area-of-applicability",
    "href": "day4.html#transferrability-of-models-area-of-applicability",
    "title": "\n4  Machine Learning methods applied to spatial data\n",
    "section": "\n4.3 Transferrability of models: “area of applicability”",
    "text": "4.3 Transferrability of models: “area of applicability”\nExplained here;\n\naoa &lt;- aoa(r, tr)\n# No trainDI provided.\n# Warning in seq.default(along = pkg): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = code$library): partial argument\n# match of 'along' to 'along.with'\n# Warning in seq.default(along = pkg): partial argument match of\n# 'along' to 'along.with'\n# Warning in seq.default(along = code$library): partial argument\n# match of 'along' to 'along.with'\n# note: Either no model was given or no CV was used for model training. The DI threshold is therefore based on all training data\n# Computing DI of training data...\n# \n  |                                                                \n  |                                                          |   0%\n  |                                                                \n  |                                                          |   1%\n  |                                                                \n  |=                                                         |   1%\n  |                                                                \n  |=                                                         |   2%\n  |                                                                \n  |=                                                         |   3%\n  |                                                                \n  |==                                                        |   3%\n  |                                                                \n  |==                                                        |   4%\n  |                                                                \n  |===                                                       |   4%\n  |                                                                \n  |===                                                       |   5%\n  |                                                                \n  |===                                                       |   6%\n  |                                                                \n  |====                                                      |   6%\n  |                                                                \n  |====                                                      |   7%\n  |                                                                \n  |====                                                      |   8%\n  |                                                                \n  |=====                                                     |   8%\n  |                                                                \n  |=====                                                     |   9%\n  |                                                                \n  |======                                                    |  10%\n  |                                                                \n  |======                                                    |  11%\n  |                                                                \n  |=======                                                   |  11%\n  |                                                                \n  |=======                                                   |  12%\n  |                                                                \n  |=======                                                   |  13%\n  |                                                                \n  |========                                                  |  13%\n  |                                                                \n  |========                                                  |  14%\n  |                                                                \n  |========                                                  |  15%\n  |                                                                \n  |=========                                                 |  15%\n  |                                                                \n  |=========                                                 |  16%\n  |                                                                \n  |==========                                                |  17%\n  |                                                                \n  |==========                                                |  18%\n  |                                                                \n  |===========                                               |  18%\n  |                                                                \n  |===========                                               |  19%\n  |                                                                \n  |===========                                               |  20%\n  |                                                                \n  |============                                              |  20%\n  |                                                                \n  |============                                              |  21%\n  |                                                                \n  |=============                                             |  22%\n  |                                                                \n  |=============                                             |  23%\n  |                                                                \n  |==============                                            |  23%\n  |                                                                \n  |==============                                            |  24%\n  |                                                                \n  |==============                                            |  25%\n  |                                                                \n  |===============                                           |  25%\n  |                                                                \n  |===============                                           |  26%\n  |                                                                \n  |===============                                           |  27%\n  |                                                                \n  |================                                          |  27%\n  |                                                                \n  |================                                          |  28%\n  |                                                                \n  |=================                                         |  28%\n  |                                                                \n  |=================                                         |  29%\n  |                                                                \n  |=================                                         |  30%\n  |                                                                \n  |==================                                        |  30%\n  |                                                                \n  |==================                                        |  31%\n  |                                                                \n  |==================                                        |  32%\n  |                                                                \n  |===================                                       |  32%\n  |                                                                \n  |===================                                       |  33%\n  |                                                                \n  |===================                                       |  34%\n  |                                                                \n  |====================                                      |  34%\n  |                                                                \n  |====================                                      |  35%\n  |                                                                \n  |=====================                                     |  35%\n  |                                                                \n  |=====================                                     |  36%\n  |                                                                \n  |=====================                                     |  37%\n  |                                                                \n  |======================                                    |  37%\n  |                                                                \n  |======================                                    |  38%\n  |                                                                \n  |======================                                    |  39%\n  |                                                                \n  |=======================                                   |  39%\n  |                                                                \n  |=======================                                   |  40%\n  |                                                                \n  |========================                                  |  41%\n  |                                                                \n  |========================                                  |  42%\n  |                                                                \n  |=========================                                 |  42%\n  |                                                                \n  |=========================                                 |  43%\n  |                                                                \n  |=========================                                 |  44%\n  |                                                                \n  |==========================                                |  44%\n  |                                                                \n  |==========================                                |  45%\n  |                                                                \n  |==========================                                |  46%\n  |                                                                \n  |===========================                               |  46%\n  |                                                                \n  |===========================                               |  47%\n  |                                                                \n  |============================                              |  48%\n  |                                                                \n  |============================                              |  49%\n  |                                                                \n  |=============================                             |  49%\n  |                                                                \n  |=============================                             |  50%\n  |                                                                \n  |=============================                             |  51%\n  |                                                                \n  |==============================                            |  51%\n  |                                                                \n  |==============================                            |  52%\n  |                                                                \n  |===============================                           |  53%\n  |                                                                \n  |===============================                           |  54%\n  |                                                                \n  |================================                          |  54%\n  |                                                                \n  |================================                          |  55%\n  |                                                                \n  |================================                          |  56%\n  |                                                                \n  |=================================                         |  56%\n  |                                                                \n  |=================================                         |  57%\n  |                                                                \n  |=================================                         |  58%\n  |                                                                \n  |==================================                        |  58%\n  |                                                                \n  |==================================                        |  59%\n  |                                                                \n  |===================================                       |  60%\n  |                                                                \n  |===================================                       |  61%\n  |                                                                \n  |====================================                      |  61%\n  |                                                                \n  |====================================                      |  62%\n  |                                                                \n  |====================================                      |  63%\n  |                                                                \n  |=====================================                     |  63%\n  |                                                                \n  |=====================================                     |  64%\n  |                                                                \n  |=====================================                     |  65%\n  |                                                                \n  |======================================                    |  65%\n  |                                                                \n  |======================================                    |  66%\n  |                                                                \n  |=======================================                   |  66%\n  |                                                                \n  |=======================================                   |  67%\n  |                                                                \n  |=======================================                   |  68%\n  |                                                                \n  |========================================                  |  68%\n  |                                                                \n  |========================================                  |  69%\n  |                                                                \n  |========================================                  |  70%\n  |                                                                \n  |=========================================                 |  70%\n  |                                                                \n  |=========================================                 |  71%\n  |                                                                \n  |=========================================                 |  72%\n  |                                                                \n  |==========================================                |  72%\n  |                                                                \n  |==========================================                |  73%\n  |                                                                \n  |===========================================               |  73%\n  |                                                                \n  |===========================================               |  74%\n  |                                                                \n  |===========================================               |  75%\n  |                                                                \n  |============================================              |  75%\n  |                                                                \n  |============================================              |  76%\n  |                                                                \n  |============================================              |  77%\n  |                                                                \n  |=============================================             |  77%\n  |                                                                \n  |=============================================             |  78%\n  |                                                                \n  |==============================================            |  79%\n  |                                                                \n  |==============================================            |  80%\n  |                                                                \n  |===============================================           |  80%\n  |                                                                \n  |===============================================           |  81%\n  |                                                                \n  |===============================================           |  82%\n  |                                                                \n  |================================================          |  82%\n  |                                                                \n  |================================================          |  83%\n  |                                                                \n  |=================================================         |  84%\n  |                                                                \n  |=================================================         |  85%\n  |                                                                \n  |==================================================        |  85%\n  |                                                                \n  |==================================================        |  86%\n  |                                                                \n  |==================================================        |  87%\n  |                                                                \n  |===================================================       |  87%\n  |                                                                \n  |===================================================       |  88%\n  |                                                                \n  |===================================================       |  89%\n  |                                                                \n  |====================================================      |  89%\n  |                                                                \n  |====================================================      |  90%\n  |                                                                \n  |=====================================================     |  91%\n  |                                                                \n  |=====================================================     |  92%\n  |                                                                \n  |======================================================    |  92%\n  |                                                                \n  |======================================================    |  93%\n  |                                                                \n  |======================================================    |  94%\n  |                                                                \n  |=======================================================   |  94%\n  |                                                                \n  |=======================================================   |  95%\n  |                                                                \n  |=======================================================   |  96%\n  |                                                                \n  |========================================================  |  96%\n  |                                                                \n  |========================================================  |  97%\n  |                                                                \n  |========================================================= |  97%\n  |                                                                \n  |========================================================= |  98%\n  |                                                                \n  |========================================================= |  99%\n  |                                                                \n  |==========================================================|  99%\n  |                                                                \n  |==========================================================| 100%\n# Computing DI of new data...\n# Computing AOA...\n# Warning in trainDI$thres: partial match of 'thres' to 'threshold'\n# Finished!\nplot(aoa)\n# Warning: Removed 395 rows containing non-finite outside the scale range\n# (`stat_density()`).\n\n\n\n\n\n\nplot(aoa$DI)\n\n\n\n\n\n\nplot(aoa$AOA)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning methods applied to spatial data</span>"
    ]
  },
  {
    "objectID": "day4.html#random-forests-for-spatially-dependent-data",
    "href": "day4.html#random-forests-for-spatially-dependent-data",
    "title": "\n4  Machine Learning methods applied to spatial data\n",
    "section": "\n4.4 Random Forests for Spatially Dependent Data",
    "text": "4.4 Random Forests for Spatially Dependent Data\nR package RandomForestGLS!\nCombines the good parts of RF and Gaussian processes, in a very smart way! (final paper, paywalled, here). The discussion on variable selection / variable importance under spatial correlated residuals is worth reading.\n\nlibrary(RandomForestsGLS)\ncc = st_coordinates(splotdata)\nload(\"rfgls.rda\")\nif (!exists(\"rfgls\")) {\n     rfgls = RFGLS_estimate_spatial(cc, as.double(y), x)\n}\ncc_pr = st_coordinates(split(r))\nhead(as.data.frame(split(r)))\n#       x     y bio_1 bio_4 bio_5 bio_6 bio_8 bio_9 bio_12 bio_13\n# 1 -75.6 -17.6    NA    NA    NA    NA    NA    NA     NA     NA\n# 2 -75.5 -17.6    NA    NA    NA    NA    NA    NA     NA     NA\n# 3 -75.5 -17.6    NA    NA    NA    NA    NA    NA     NA     NA\n# 4 -75.4 -17.6    NA    NA    NA    NA    NA    NA     NA     NA\n# 5 -75.3 -17.6    NA    NA    NA    NA    NA    NA     NA     NA\n# 6 -75.2 -17.6    NA    NA    NA    NA    NA    NA     NA     NA\n#   bio_14 bio_15 elev\n# 1     NA     NA   NA\n# 2     NA     NA   NA\n# 3     NA     NA   NA\n# 4     NA     NA   NA\n# 5     NA     NA   NA\n# 6     NA     NA   NA\npr = RFGLS_predict_spatial(rfgls, as.matrix(cc_pr), as.data.frame(split(r))[-(1:2)])\n# Warning in BRISC_estimation(coords, x = matrix(1, nrow(coords), 1), y = rfgls_residual, : The ordering of inputs x (covariates) and y (response) in BRISC_estimation has been changed BRISC 1.0.0 onwards.\n# Please check the new documentation with ?BRISC_estimation.\nout = split(r)\nout$rfgls = pr$prediction\nout$rf = predict(split(r), tr)\nplot(merge(out[c(\"rf\", \"rfgls\")]), breaks = \"equal\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Machine Learning methods applied to spatial data</span>"
    ]
  },
  {
    "objectID": "day5.html",
    "href": "day5.html",
    "title": "\n5  Analysing lattice data; big geospatial datasets\n",
    "section": "",
    "text": "Reading materials\nFrom Spatial Data Science: with applications in R:\nstars vignettes 2: proxy objects",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysing lattice data; big geospatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#exercises-for-today",
    "href": "day5.html#exercises-for-today",
    "title": "\n5  Analysing lattice data; big geospatial datasets\n",
    "section": "\n5.1 Exercises for Today",
    "text": "5.1 Exercises for Today\n\nExercises of Ch 9: Big Data and Cloud Native\n\n\n\n\n\n\n\nSummary\n\n\n\n\nWhat is big?\nRaster or vector?\nHow to access large data sets?\nSpatial statistics on large datasets",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysing lattice data; big geospatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#analysing-lattice-data-neighbours-weights-models",
    "href": "day5.html#analysing-lattice-data-neighbours-weights-models",
    "title": "\n5  Analysing lattice data; big geospatial datasets\n",
    "section": "\n5.2 Analysing lattice data: neighbours, weights, models",
    "text": "5.2 Analysing lattice data: neighbours, weights, models\n\nlibrary(sf)\n# Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE\ndata(pol_pres15, package = \"spDataLarge\")\npol_pres15 |&gt;\n    subset(select = c(TERYT, name, types)) |&gt;\n    head()\n# Simple feature collection with 6 features and 3 fields\n# Geometry type: MULTIPOLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 235000 ymin: 367000 xmax: 281000 ymax: 413000\n# Projected CRS: ETRF2000-PL / CS92\n#    TERYT                name       types\n# 1 020101         BOLESŁAWIEC       Urban\n# 2 020102         BOLESŁAWIEC       Rural\n# 3 020103            GROMADKA       Rural\n# 4 020104        NOWOGRODZIEC Urban/rural\n# 5 020105          OSIECZNICA       Rural\n# 6 020106 WARTA BOLESŁAWIECKA       Rural\n#                         geometry\n# 1 MULTIPOLYGON (((261089 3855...\n# 2 MULTIPOLYGON (((254150 3837...\n# 3 MULTIPOLYGON (((275346 3846...\n# 4 MULTIPOLYGON (((251770 3770...\n# 5 MULTIPOLYGON (((263424 4060...\n# 6 MULTIPOLYGON (((267031 3870...\nlibrary(tmap, warn.conflicts = FALSE)\ntm_shape(pol_pres15) + tm_fill(\"types\")\n\n\n\n\n\n\n\nWe need to make the geometries valid first,\n\nst_is_valid(pol_pres15) |&gt; all()\n# [1] FALSE\npol_pres15 &lt;- st_make_valid(pol_pres15)\nst_is_valid(pol_pres15) |&gt; all()\n# [1] TRUE\n\nFirst, we will consider polygons in relationship to their direct neighbours\n\nlibrary(spdep)\n# Loading required package: spData\npol_pres15 |&gt; poly2nb(queen = TRUE) -&gt; nb_q\nnb_q\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 14242 \n# Percentage nonzero weights: 0.229 \n# Average number of links: 5.71\n\nIs the graph connected?\n\n(nb_q |&gt; n.comp.nb())$nc\n# [1] 1\n\n\npar(mar = rep(0, 4))\npol_pres15 |&gt;\n    st_geometry() |&gt;\n    st_centroid(of_largest_polygon = TRUE) -&gt; coords\nplot(st_geometry(pol_pres15), border = 'grey')\nplot(nb_q, coords = coords, add = TRUE, points = FALSE)\n\n\n\n\n\n\n\nAlternative approaches to form neighbourhood matrices:\n\nbased on distance\nbased on triangulating points, for instance polygon centroids\nsphere of influence, a modification of triangulation\ninclude neighbours from neighbours\n\nWeights matrices\nWeight matrices are needed in analysis, they determine how observations (or residuals) are weighted in a regression model.\n\n(nb_q |&gt; nb2listw(style = \"B\") -&gt; lw_q_B)\n# Characteristics of weights list object:\n# Neighbour list object:\n# Number of regions: 2495 \n# Number of nonzero links: 14242 \n# Percentage nonzero weights: 0.229 \n# Average number of links: 5.71 \n# \n# Weights style: B \n# Weights constants summary:\n#      n      nn    S0    S1     S2\n# B 2495 6225025 14242 28484 357280\n\nSpatial correlation: Moran’s I\nMoran’s I is defined as\n\\[\nI = \\frac{n \\sum_{(2)} w_{ij} z_i z_j}{S_0 \\sum_{i=1}^{n} z_i^2}\n\\] where \\(x_i, i=1, \\ldots, n\\) are \\(n\\) observations on the numeric variable of interest, \\(z_i = x_i - \\bar{x}\\), \\(\\bar{x} = \\sum_{i=1}^{n} x_i / n\\), \\(\\sum_{(2)} = \\stackrel{\\sum_{i=1}^{n} \\sum_{j=1}^{n}}{i \\neq j}\\), \\(w_{ij}\\) are the spatial weights, and \\(S_0 = \\sum_{(2)} w_{ij}\\).\nWe can compute it as\n\npol_pres15$I_turnout |&gt;\n    moran.test(lw_q_B, randomisation = FALSE,\n               alternative = \"two.sided\")\n# \n#   Moran I test under normality\n# \n# data:  pol_pres15$I_turnout  \n# weights: lw_q_B    \n# \n# Moran I statistic standard deviate = 58, p-value &lt;2e-16\n# alternative hypothesis: two.sided\n# sample estimates:\n# Moran I statistic       Expectation          Variance \n#          0.691434         -0.000401          0.000140\nplot(pol_pres15[\"I_turnout\"])\n\n\n\n\n\n\n\n\nsummary(pol_pres15$I_entitled_to_vote)\n#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#    1308    4026    6033   12221   10524  594643\n(lm0 &lt;- lm(I_turnout ~ I_entitled_to_vote, pol_pres15)) |&gt; summary()\n# \n# Call:\n# lm(formula = I_turnout ~ I_entitled_to_vote, data = pol_pres15)\n# \n# Residuals:\n#      Min       1Q   Median       3Q      Max \n# -0.21352 -0.04387 -0.00092  0.04150  0.23611 \n# \n# Coefficients:\n#                    Estimate Std. Error t value Pr(&gt;|t|)    \n# (Intercept)        4.39e-01   1.34e-03   328.1   &lt;2e-16 ***\n# I_entitled_to_vote 5.26e-07   4.18e-08    12.6   &lt;2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 0.0618 on 2493 degrees of freedom\n# Multiple R-squared:  0.0598,  Adjusted R-squared:  0.0595 \n# F-statistic:  159 on 1 and 2493 DF,  p-value: &lt;2e-16\npol_pres15$res = residuals(lm0)\nplot(pol_pres15[\"res\"])",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysing lattice data; big geospatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#big-data-resource-constraints-in-data-science-projects",
    "href": "day5.html#big-data-resource-constraints-in-data-science-projects",
    "title": "\n5  Analysing lattice data; big geospatial datasets\n",
    "section": "\n5.3 Big data: resource constraints in data science projects",
    "text": "5.3 Big data: resource constraints in data science projects\nConstraints concern the availability of:\n\ntime (your time, time of team members)\ncompute (pc’s, cluster, private cloud)\nmoney (e.g. to hire and/or (re)train people, or to rent public cloud infrastructure)\n\nPublic clouds provide:\n\ninfinite (in practice) compute\ninfinite (in practice) storage\n\nbut cost\n\nhard money to use (compute, storage, network/data access)\npeople capacity to setup and maintain\n\nThere is no cloud!\nit’s just someone else’s computer!\n\nwhich is true: the computers have a different shape, but are just like your laptop:\n\nthey have a CPU, main memory, hard drive, possibly a GPU\nquite often you will find yourself on a virtual machine, which acts as a normal computer\nbut see below: they have object storage!",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysing lattice data; big geospatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#what-is-a-big-dataset",
    "href": "day5.html#what-is-a-big-dataset",
    "title": "\n5  Analysing lattice data; big geospatial datasets\n",
    "section": "\n5.4 What is a big dataset?",
    "text": "5.4 What is a big dataset?\n\nWhat is big?\n\ntoo big to handle in main memory (with some copying) (Gb)\ntoo big to fit in memory (20 Gb)\ntoo big to download (Tb)\ntoo big to fit on the hard drive, or local file storage (10 Tb)\ntoo big to move (copy) to your institution (100 Tb - Pb)\n\n\n\n\n\n\n\n\n\nBreakout session 1\n\n\n\nDiscuss:\n\nHave you used datasets obtained from cloud storage? For which case(s)?\nHave you used cloud processing? For which case(s)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysing lattice data; big geospatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#r-for-big-tabular-datasets",
    "href": "day5.html#r-for-big-tabular-datasets",
    "title": "\n5  Analysing lattice data; big geospatial datasets\n",
    "section": "\n5.5 R for big, tabular datasets",
    "text": "5.5 R for big, tabular datasets\n\nIn-memory solutions: data.table, duckdb, polars improve speed (use indexes)\nOut-of-memory solution: DBI or tidyverse via dbplyr, connect to\n\na local, on-disc database like MariaDB, PostgreSQL, or MySQL\ncloud-based databases like Google BigQuery, Snowflake,",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysing lattice data; big geospatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#big-geospatial",
    "href": "day5.html#big-geospatial",
    "title": "\n5  Analysing lattice data; big geospatial datasets\n",
    "section": "\n5.6 Big geospatial",
    "text": "5.6 Big geospatial\n\nLarge vector datasets, examples:\n\nall building footprints of a continents, link\n\nall rivers, e.g. of the US, link\n\nOpenStreetMap, link\n\nall agricultural parcels of a continent, e.g. EuroCrops\n\n\n\nLarge raster datasets, image collections and data cubes:\n\nERA-5\n\nCMIP-6, partly on google and AWS\n\nCopernicus (Sentinel-1, 2, 3, 5p, etc), e.g. on CDSE\n\nLandsat, MODIS, … download\n\n\n\nCloud solutions, cloud platforms, with platform lock-in:\n\nArcGIS online\nSentinel Hub\nGoogle Earth Engine\nMicrosoft Planetary Computer\n\nEarth on Amazon (AWS US-west Oregon: COGS + STAC for S1 + S2)\nCopernicus Data Space Ecosystem (but has openEO: a fully open standard and open source software stack)\n\n\n\n\n\n\n\n\n\nClouds and object storage\n\n\n\nObject storage abstracts away hard drives and file systems!\n\ne.g. S3 bucket (AWS/OpenStack):\n\ntotal size is unlimited\n\nmaximum object size 5 Tb (AWS S3)\nidea: write once, read many times\nlarge objects: write piece-wise\nhttp range requests\nprice depends on size, access speed, amount of requests\ntabular data: Parquet\n\n\nlarge data processing: collocate processing and storage\n\navoid network between locations / data centers\nnetwork inside a data center is fast / cheap",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysing lattice data; big geospatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#access-mechanism",
    "href": "day5.html#access-mechanism",
    "title": "\n5  Analysing lattice data; big geospatial datasets\n",
    "section": "\n5.7 Access mechanism",
    "text": "5.7 Access mechanism\n\nAPI:\n\nprocess: openEO cloud, openEO on CDSE,\nselect, download, process: Climate Data Store\n\nfind “assets” (files): STAC, stacindex\n\n\n\npartial reads of data cubes: variable, bounding box, strided (low resolution), time period\nvector tiles: pmtiles, flatgeobuf\n\n\n\n\n\n\n\nCloud-optimized, cloud-native geoospatial\n\n\n\n\nCloud-optimized formats let you read sections of large, remote files using HTTP range requests\nexamples: Cloud-optimized GeoTIFF (COG), GeoZarr, GeoParquet\nThese are described in the Cloud-Optimized Geospatial Formats Guide",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysing lattice data; big geospatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#examples-openeo",
    "href": "day5.html#examples-openeo",
    "title": "\n5  Analysing lattice data; big geospatial datasets\n",
    "section": "\n5.8 Examples openEO",
    "text": "5.8 Examples openEO\nTwo video’s from me taken during the 2023 OpenGeoHub Summerschool, on the topic “Cloud-based analysis of Earth Observation data using openEO Platform, R and Python” can be found here:\n\npart 1\npart 2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysing lattice data; big geospatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#example-rstac",
    "href": "day5.html#example-rstac",
    "title": "\n5  Analysing lattice data; big geospatial datasets\n",
    "section": "\n5.9 Example rstac\n",
    "text": "5.9 Example rstac\n\nUsing Sentinel-2 COGs at AWS, and its stac:\n\nlibrary(rstac) # modified from the package docs:\ns_obj = stac(\"https://earth-search.aws.element84.com/v1\")\ncollections(s_obj) |&gt; get_request()\n# ###Collections\n# - collections (9 item(s)):\n#   - sentinel-2-pre-c1-l2a\n#   - cop-dem-glo-30\n#   - naip\n#   - cop-dem-glo-90\n#   - landsat-c2-l2\n#   - sentinel-2-l2a\n#   - sentinel-2-l1c\n#   - sentinel-2-c1-l2a\n#   - sentinel-1-grd\n# - field(s): collections, links, context\nit_obj &lt;- s_obj |&gt;\n  stac_search(collections = \"sentinel-2-l2a\",\n              bbox = c(-47.02148, -17.35063, -42.53906, -12.98314),\n              datetime = \"2022-02-12T00:00:00Z/2022-03-18T00:00:00Z\",\n              limit = 1) |&gt; \n  get_request()\nit_obj\n# ###Items\n# - matched feature(s): 361\n# - features (1 item(s) / 360 not fetched):\n#   - S2A_23KMA_20220317_0_L2A\n# - assets: \n# aot, aot-jp2, blue, blue-jp2, coastal, coastal-jp2, granule_metadata, green, green-jp2, nir, nir-jp2, nir08, nir08-jp2, nir09, nir09-jp2, red, red-jp2, rededge1, rededge1-jp2, rededge2, rededge2-jp2, rededge3, rededge3-jp2, scl, scl-jp2, swir16, swir16-jp2, swir22, swir22-jp2, thumbnail, tileinfo_metadata, visual, visual-jp2, wvp, wvp-jp2\n# - item's fields: \n# assets, bbox, collection, geometry, id, links, properties, stac_extensions, stac_version, type\n\nthen, download (here only one item):\n\ndownload_items &lt;- it_obj |&gt;\n  assets_download(assets_name = \"thumbnail\", items_max = 1, overwrite = TRUE)\n\nand examine\n\nlibrary(sf)\ntif = \"sentinel-s2-l2a-cogs/23/K/MA/2022/3/S2A_23KMA_20220317_0_L2A/B04.tif\"\ngdal_utils(\"info\", tif)\n# Driver: GTiff/GeoTIFF\n# Files: sentinel-s2-l2a-cogs/23/K/MA/2022/3/S2A_23KMA_20220317_0_L2A/B04.tif\n# Size is 10980, 10980\n# Coordinate System is:\n# PROJCRS[\"WGS 84 / UTM zone 23S\",\n#     BASEGEOGCRS[\"WGS 84\",\n#         ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n#             MEMBER[\"World Geodetic System 1984 (Transit)\"],\n#             MEMBER[\"World Geodetic System 1984 (G730)\"],\n#             MEMBER[\"World Geodetic System 1984 (G873)\"],\n#             MEMBER[\"World Geodetic System 1984 (G1150)\"],\n#             MEMBER[\"World Geodetic System 1984 (G1674)\"],\n#             MEMBER[\"World Geodetic System 1984 (G1762)\"],\n#             MEMBER[\"World Geodetic System 1984 (G2139)\"],\n#             ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n#                 LENGTHUNIT[\"metre\",1]],\n#             ENSEMBLEACCURACY[2.0]],\n#         PRIMEM[\"Greenwich\",0,\n#             ANGLEUNIT[\"degree\",0.0174532925199433]],\n#         ID[\"EPSG\",4326]],\n#     CONVERSION[\"UTM zone 23S\",\n#         METHOD[\"Transverse Mercator\",\n#             ID[\"EPSG\",9807]],\n#         PARAMETER[\"Latitude of natural origin\",0,\n#             ANGLEUNIT[\"degree\",0.0174532925199433],\n#             ID[\"EPSG\",8801]],\n#         PARAMETER[\"Longitude of natural origin\",-45,\n#             ANGLEUNIT[\"degree\",0.0174532925199433],\n#             ID[\"EPSG\",8802]],\n#         PARAMETER[\"Scale factor at natural origin\",0.9996,\n#             SCALEUNIT[\"unity\",1],\n#             ID[\"EPSG\",8805]],\n#         PARAMETER[\"False easting\",500000,\n#             LENGTHUNIT[\"metre\",1],\n#             ID[\"EPSG\",8806]],\n#         PARAMETER[\"False northing\",10000000,\n#             LENGTHUNIT[\"metre\",1],\n#             ID[\"EPSG\",8807]]],\n#     CS[Cartesian,2],\n#         AXIS[\"(E)\",east,\n#             ORDER[1],\n#             LENGTHUNIT[\"metre\",1]],\n#         AXIS[\"(N)\",north,\n#             ORDER[2],\n#             LENGTHUNIT[\"metre\",1]],\n#     USAGE[\n#         SCOPE[\"Navigation and medium accuracy spatial referencing.\"],\n#         AREA[\"Between 48°W and 42°W, southern hemisphere between 80°S and equator, onshore and offshore. Brazil.\"],\n#         BBOX[-80,-48,0,-42]],\n#     ID[\"EPSG\",32723]]\n# Data axis to CRS axis mapping: 1,2\n# Origin = (399960.000000000000000,8100040.000000000000000)\n# Pixel Size = (10.000000000000000,-10.000000000000000)\n# Metadata:\n#   AREA_OR_POINT=Area\n#   OVR_RESAMPLING_ALG=AVERAGE\n# Image Structure Metadata:\n#   COMPRESSION=DEFLATE\n#   INTERLEAVE=BAND\n#   PREDICTOR=2\n# Corner Coordinates:\n# Upper Left  (  399960.000, 8100040.000) ( 45d56'26.60\"W, 17d10'56.12\"S)\n# Lower Left  (  399960.000, 7990240.000) ( 45d56'45.24\"W, 18d10'28.55\"S)\n# Upper Right (  509760.000, 8100040.000) ( 44d54'29.58\"W, 17d11' 3.94\"S)\n# Lower Right (  509760.000, 7990240.000) ( 44d54'27.77\"W, 18d10'36.85\"S)\n# Center      (  454860.000, 8045140.000) ( 45d25'32.30\"W, 17d40'48.86\"S)\n# Band 1 Block=1024x1024 Type=UInt16, ColorInterp=Gray\n#   NoData Value=0\n#   Overviews: 5490x5490, 2745x2745, 1373x1373, 687x687\nlibrary(stars)\n# Loading required package: abind\nread_stars(tif) |&gt; plot()\n# downsample set to 8\n\n\n\n\n\n\n\n\n\n\n\n\n\nBreakout session 2\n\n\n\nDiscuss:\n\nHave you used any cloud platforms for processing geospatial data?\nWhat is your position with respect to platform lock-in?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysing lattice data; big geospatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#further-examples-from-r-spatial.org",
    "href": "day5.html#further-examples-from-r-spatial.org",
    "title": "\n5  Analysing lattice data; big geospatial datasets\n",
    "section": "\n5.10 Further examples from r-spatial.org:",
    "text": "5.10 Further examples from r-spatial.org:\n\nCloud-based processing of satellite image collections in R using STAC, COGs, and on-demand data cubes\nReading Zarr files with R package stars\nProcessing large scale satellite imagery with openEO Platform and R",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysing lattice data; big geospatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#examples-vsixxx",
    "href": "day5.html#examples-vsixxx",
    "title": "\n5  Analysing lattice data; big geospatial datasets\n",
    "section": "\n5.11 Examples /vsixxx\n",
    "text": "5.11 Examples /vsixxx\n\n\ncurl::curl_download(\n  \"https://github.com/paleolimbot/geoarrow-data/releases/download/v0.0.1/nshn_water_line.gpkg\",\n  \"nshn_water_line.gpkg\"\n)\n\n\nlibrary(sf)\n\n\n(w &lt;- read_sf(\"nshn_water_line.gpkg\"))\n# Simple feature collection with 483268 features and 33 fields\n# Geometry type: MULTILINESTRING\n# Dimension:     XYZ\n# Bounding box:  xmin: 216000 ymin: 4790000 xmax: 782000 ymax: 5240000\n# z_range:       zmin: -41.7 zmax: 530\n# Projected CRS: NAD83 / UTM zone 20N\n# # A tibble: 483,268 × 34\n#   OBJECTID FEAT_CODE ZVALUE PLANLENGTH  MINZ  MAXZ LINE_CLASS\n#      &lt;dbl&gt; &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;      &lt;int&gt;\n# 1        2 WACO20       0.2      280.    0.2   0.2          3\n# 2        3 WACO20       0.2      185.    0.2   0.2          3\n# 3        4 WACO20       0.2      179.    0.2   0.2          3\n# 4        5 WACO20       0.2     1779.    0.2   0.2          3\n# 5        6 WACO20       0.2      470.    0.2   0.2          3\n# 6        7 WACO20       0.2       57.7   0.2   0.2          3\n# # ℹ 483,262 more rows\n# # ℹ 27 more variables: FLOWDIR &lt;int&gt;, LEVELPRIOR &lt;int&gt;,\n# #   LAKEID_1 &lt;chr&gt;, LAKENAME_1 &lt;chr&gt;, LAKEID_2 &lt;chr&gt;,\n# #   LAKENAME_2 &lt;chr&gt;, RIVID_1 &lt;chr&gt;, RIVNAME_1 &lt;chr&gt;,\n# #   RIVID_2 &lt;chr&gt;, RIVNAME_2 &lt;chr&gt;, MISCID_1 &lt;chr&gt;,\n# #   MISCNAME_1 &lt;chr&gt;, MISCID_2 &lt;chr&gt;, MISCNAME_2 &lt;chr&gt;,\n# #   MISCID_3 &lt;chr&gt;, MISCNAME_3 &lt;chr&gt;, MISCID_4 &lt;chr&gt;, …\n\nFrom https://github.com/microsoft/USBuildingFootprints downloaded Maine.geojson.zip, and read with\n\n(m = read_sf(\"/vsizip/Maine.geojson.zip\")) # /vsizip: indicates data source is a zipped file\n# Simple feature collection with 758999 features and 2 fields\n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: -71.1 ymin: 43 xmax: -67 ymax: 47.5\n# Geodetic CRS:  WGS 84\n# # A tibble: 758,999 × 3\n#   release capture_dates_range                               geometry\n#     &lt;int&gt; &lt;chr&gt;                                        &lt;POLYGON [°]&gt;\n# 1       1 \"\"                  ((-67 44.9, -67 44.9, -67 44.9, -67 4…\n# 2       1 \"\"                  ((-67 44.9, -67 44.9, -67 44.9, -67 4…\n# 3       1 \"\"                  ((-67 44.9, -67 44.9, -67 44.9, -67 4…\n# 4       1 \"\"                  ((-67 44.9, -67 44.9, -67 44.9, -67 4…\n# 5       1 \"\"                  ((-67 44.9, -67 44.9, -67 44.9, -67 4…\n# 6       1 \"\"                  ((-67 44.9, -67 44.9, -67 44.9, -67 4…\n# # ℹ 758,993 more rows\n\nor read directly from github into R:\n\nm = st_read(\"/vsizip/vsicurl/https://minedbuildings.z5.web.core.windows.net/legacy/usbuildings-v2/Maine.geojson.zip\")\n# /vsicurl: indicates data source is a URL",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysing lattice data; big geospatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#simple-analysis-on-large-datasets",
    "href": "day5.html#simple-analysis-on-large-datasets",
    "title": "\n5  Analysing lattice data; big geospatial datasets\n",
    "section": "\n5.12 “Simple” analysis on large datasets",
    "text": "5.12 “Simple” analysis on large datasets\n\nprocess full archives, compute in the cloud\nselect subsets, download, process locally:\n\nspatial subset\ntemporal subset\nsampled at lower resolution (spatially, temporally)\naggregated (=processed?) to lower resolution\n\n\nin some disciplines (Earth Observation?) there seems to be a belief that processing at the full resolution is the only thing that produces real science\nthere is surprisingly little literature on the loss of information when processing at lower resolution, e.g. when the goal is to create a curve of yearly deforestation over an area as large as Brazil",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysing lattice data; big geospatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#spatial-statistics-on-large-datasets",
    "href": "day5.html#spatial-statistics-on-large-datasets",
    "title": "\n5  Analysing lattice data; big geospatial datasets\n",
    "section": "\n5.13 Spatial statistics on large datasets",
    "text": "5.13 Spatial statistics on large datasets\nGeostatistics\n\nRandomForestsGLS\nspNNGP\nFRK\n\nA key paper comparing different approaches is Heaton, Matthew J., Abhirup Datta, Andrew O. Finley, Reinhard Furrer, Joseph Guinness, Rajarshi Guhaniyogi, Florian Gerber, et al. 2018. “A Case Study Competition Among Methods for Analyzing Large Spatial Data.” Journal of Agricultural, Biological and Environmental Statistics, December. DOI.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysing lattice data; big geospatial datasets</span>"
    ]
  },
  {
    "objectID": "day5.html#if-time-is-left",
    "href": "day5.html#if-time-is-left",
    "title": "\n5  Analysing lattice data; big geospatial datasets\n",
    "section": "\n5.14 If time is left",
    "text": "5.14 If time is left\n\nExercises chapter 9",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Analysing lattice data; big geospatial datasets</span>"
    ]
  }
]